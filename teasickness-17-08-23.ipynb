{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Initialization","metadata":{}},{"cell_type":"markdown","source":"#### 1.1 Importing libraries","metadata":{}},{"cell_type":"code","source":"!pip install visual-attention-tf","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:53:56.753178Z","iopub.execute_input":"2023-09-23T17:53:56.753494Z","iopub.status.idle":"2023-09-23T17:54:11.292655Z","shell.execute_reply.started":"2023-09-23T17:53:56.753468Z","shell.execute_reply":"2023-09-23T17:54:11.291432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom numpy import cov\nfrom numpy import trace\nfrom numpy import iscomplexobj\nfrom numpy import asarray\nfrom numpy.random import randint\nfrom scipy.linalg import sqrtm\nimport tensorflow_addons as tfa\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport PIL\nimport skimage\nfrom skimage.exposure import is_low_contrast\nfrom skimage.transform import rescale, SimilarityTransform, AffineTransform, rotate\n# comment for kaggle/colab\n##########################\n# import mediapipe as mp\n# import Augmentor\n# from sympy import im\n##########################\nimport tensorflow as tf\n# import tensorflow_addons as tfa\n# import tensorflow_gan as tfg\nfrom tensorflow import keras\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras import layers, losses\nfrom keras.layers import Input, InputLayer,  Dense, Embedding, Conv2D, Dropout, Flatten, RandomTranslation, LeakyReLU, Reshape, Conv2DTranspose, GlobalMaxPooling2D\nfrom keras.models import Sequential, Model\nfrom keras import activations\nfrom tensorflow.keras.applications import DenseNet121, resnet50, MobileNetV2, VGG19, InceptionV3\nimport tensorflow_hub as hub\nimport tensorflow_probability as tfp\nfrom PIL import Image, ImageFont, ImageDraw, ImageColor\nimport os\nimport pathlib\nimport tarfile\nimport pathlib as pb\nimport pandas as pd\nfrom collections import defaultdict\nimport enum\nimport random\nimport seaborn as sns\nfrom tqdm import tqdm\nimport datetime\nimport shutil\n# comment for kaggle/colab\n# from rembg import remove\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing, dummy\nimport warnings\nimport time\nfrom IPython.display import FileLinks, FileLink\nfrom visual_attention import PixelAttention2D , ChannelAttention2D,EfficientChannelAttention2D\n\n\nsns.set_style('dark')\nnp.random.seed(42)\nwarnings.filterwarnings('ignore')\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:11.296220Z","iopub.execute_input":"2023-09-23T17:54:11.296589Z","iopub.status.idle":"2023-09-23T17:54:22.858461Z","shell.execute_reply.started":"2023-09-23T17:54:11.296560Z","shell.execute_reply":"2023-09-23T17:54:22.857501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2 Libraries version check","metadata":{}},{"cell_type":"code","source":"tf.__version__, tf.executing_eagerly(), np.__version__, pd.__version__\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:22.859773Z","iopub.execute_input":"2023-09-23T17:54:22.860521Z","iopub.status.idle":"2023-09-23T17:54:22.873666Z","shell.execute_reply.started":"2023-09-23T17:54:22.860481Z","shell.execute_reply":"2023-09-23T17:54:22.871334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:22.877432Z","iopub.execute_input":"2023-09-23T17:54:22.878935Z","iopub.status.idle":"2023-09-23T17:54:23.905798Z","shell.execute_reply.started":"2023-09-23T17:54:22.878886Z","shell.execute_reply":"2023-09-23T17:54:23.904661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3 Declaring constants","metadata":{}},{"cell_type":"code","source":"kaggle = True\n\nDATASET_DIRECTORY = '../Dataset/tea sickness dataset/'\nif kaggle:\n    DATASET_DIRECTORY = '../input/tea-dataset/tea sickness dataset - kaggle'\n\n\nINCEPTION_TFHUB = 'https://tfhub.dev/tensorflow/tfgan/eval/inception/1'\n\n\nOBJECT_DETECTION_MODEL_DICT = {'ssd': {'dir': './object_detction_models/mobilenet_ssd',\n                                       'name': 'openimages_v4_ssd_mobilenet_v2_1.tar.gz'},\n\n                               'rcnn': {'dir': './object_detction_models/inception_resnet',\n                                        'name': 'faster_rcnn_openimages_v4_inception_resnet_v2_1.tar.gz'}\n                               }\nMODEL_CHECKPOINT_PATHS = {\n    'vgg': './checkpoints/vgg/',\n    'resnet': './checkpoints/resnet_50/',\n    'mobilenet': './checkpoints/mobilenet/',\n    'inception': './checkpoints/inception/',\n    'densenet': './checkpoints/densenet/',\n    'wgan': './checkpoints/wgan/',\n    'wgan_res': './checkpoints/wgan_res/',\n}\n\n\nclass model_type(enum.Enum):\n    mobilenet_ssd = 'ssd'\n    inception_resnet_rcnn = 'rcnn'\n\n\nBATCH_SIZE_DATASET = 16\nBATCH_SIZE_TRAIN = 16\nBATCH_SIZE_GAN_TRAIN = 4\nBUFFER_SIZE = 1000\nIMG_SIZE_TRAIN = (180, 180)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:23.907658Z","iopub.execute_input":"2023-09-23T17:54:23.908755Z","iopub.status.idle":"2023-09-23T17:54:23.918383Z","shell.execute_reply.started":"2023-09-23T17:54:23.908715Z","shell.execute_reply":"2023-09-23T17:54:23.917216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Reading file paths and initial analysis of images","metadata":{"tags":[]}},{"cell_type":"markdown","source":"#### 2.1 Display all the tea dataset sickness folders present","metadata":{}},{"cell_type":"code","source":"sickness_folder_lst = os.listdir(os.path.abspath(DATASET_DIRECTORY))\nsickness_folder_lst\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:23.920669Z","iopub.execute_input":"2023-09-23T17:54:23.921106Z","iopub.status.idle":"2023-09-23T17:54:23.938412Z","shell.execute_reply.started":"2023-09-23T17:54:23.921073Z","shell.execute_reply":"2023-09-23T17:54:23.937416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2 Display the number of images in each sickness folder","metadata":{}},{"cell_type":"code","source":"sickness_folder_dict = defaultdict(lambda: 'NA')\n\nfor sickenss in sickness_folder_lst:\n    folder_path = os.path.join(DATASET_DIRECTORY, sickenss)\n    img_path_lst = list(pb.Path(folder_path).glob('*.jpg'))\n    img_path_lst = [os.path.abspath(path) for path in img_path_lst]\n    sickness_folder_dict.update({sickenss: img_path_lst})\n\nimage_count_df = pd.DataFrame(index=sickness_folder_dict.keys(), data=[len(\n    v) for v in sickness_folder_dict.values()], columns=['image_count'])\nimage_count_df  # .to_clipboard()\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:23.940122Z","iopub.execute_input":"2023-09-23T17:54:23.940485Z","iopub.status.idle":"2023-09-23T17:54:24.261217Z","shell.execute_reply.started":"2023-09-23T17:54:23.940446Z","shell.execute_reply":"2023-09-23T17:54:24.260209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3 Data visualization","metadata":{}},{"cell_type":"code","source":"image_count_df.plot.bar(color='y')\nplt.show()\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:24.262811Z","iopub.execute_input":"2023-09-23T17:54:24.263150Z","iopub.status.idle":"2023-09-23T17:54:24.684386Z","shell.execute_reply.started":"2023-09-23T17:54:24.263119Z","shell.execute_reply":"2023-09-23T17:54:24.682611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def axis_plot(ax, _img_path, desc, fontsize=12):\n#     \"\"\"_summary_\n\n#     Args:\n#         ax (_type_): _description_\n#         _img_path (_type_): _description_\n#         desc (_type_): _description_\n#         fontsize (int, optional): _description_. Defaults to 12.\n#     \"\"\"\n#     img_array = cv2.imread(str(_img_path))\n\n#     ax.imshow(img_array)\n#     ax.tick_params(left=False, right=False, labelleft=False,\n#                    labelbottom=False, bottom=False)\n\n#     ax.locator_params(nbins=5)\n\n#     ax.set_title(desc, fontsize=fontsize)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.686395Z","iopub.execute_input":"2023-09-23T17:54:24.686765Z","iopub.status.idle":"2023-09-23T17:54:24.691220Z","shell.execute_reply.started":"2023-09-23T17:54:24.686729Z","shell.execute_reply":"2023-09-23T17:54:24.690363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, ax = plt.subplots(nrows=8, ncols=5, figsize=(20, 20))\n\n# for idx, key_vals in enumerate(sickness_folder_dict.items()):\n#     key, vals = key_vals\n#     rand_idxs = np.random.choice(range(len(vals)), size=5)\n#     vals = [vals[i] for i in rand_idxs]\n#     for i_dx, val in enumerate(vals):\n\n#         axis_plot(ax[idx][i_dx], val, desc=key)\n\n# plt.tight_layout(pad=0.1, w_pad=0.1, h_pad=0.25)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-09-23T17:54:24.696433Z","iopub.execute_input":"2023-09-23T17:54:24.697279Z","iopub.status.idle":"2023-09-23T17:54:24.706107Z","shell.execute_reply.started":"2023-09-23T17:54:24.697228Z","shell.execute_reply":"2023-09-23T17:54:24.705381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Image pre-processing pipeline","metadata":{}},{"cell_type":"code","source":"# class HelperFunctions():\n\n#     def __init__(self) -> None:\n#         pass\n\n#     def read_single_image(self,  _img_path='', sickness_name='', randomly=False):\n#         if randomly:\n#             _img_path = os.path.abspath(np.random.choice(\n#                 sickness_folder_dict[sickness_name]))\n\n#         else:\n#             _img_path = os.path.abspath(_img_path)\n\n#         return _img_path, cv2.imread(str(_img_path))\n\n#     def resize_image(self, img_array,  new_dsize):\n#         img_array = cv2.resize(img_array, dsize=new_dsize)\n#         return img_array\n\n#     def display_image(self, img_array, desc='', fontsize=14, figsize=(7, 7)):\n#         \"\"\"\n#         Displays an image inside the notebook.\n#         This is used by download_and_resize_image()\n#         \"\"\"\n#         fig = plt.figure(figsize=figsize)\n#         plt.grid(False)\n#         plt.tick_params(left=False, right=False, labelleft=False,\n#                         labelbottom=False, bottom=False)\n\n#         plt.title(desc, fontsize=fontsize)\n#         plt.imshow(img_array)\n\n#     def add_denoising_nlmeans(self, img_array):\n#         return cv2.fastNlMeansDenoisingColored(img_array, None, 10, 10, 7, 15)\n\n#     def add_gaussian_blur(self, img_array):\n\n#         return cv2.GaussianBlur(img_array, (5, 5), 0)\n\n#     # def add_edge_detection(img_array):\n#     #     return cv2.Canny(img_array, 30, 150)\n\n#     def create_white_bg(self, shape, bg_fp='./white_bg.jpg'):\n#         white_bg = np.full(shape, 255, dtype=np.uint8)\n#         plt.imsave(bg_fp, white_bg)\n\n#     def superimpose_white_bg(self, img, bg_fp='./white_bg.jpg', binary_mask_th=0.8):\n#         change_background_mp = mp.solutions.selfie_segmentation\n\n#         change_bg_segment = change_background_mp.SelfieSegmentation()\n#         result = change_bg_segment.process(image=img)\n#         binary_mask = result.segmentation_mask > binary_mask_th\n#         binary_mask_3 = np.dstack((binary_mask, binary_mask, binary_mask))\n\n#         output_image = np.where(binary_mask_3, img, 255)\n#         bg_img = cv2.imread(bg_fp)\n#         output_image = np.where(binary_mask_3, img, bg_img)\n#         return output_image\n\n#     def remove_bg(self, image):\n#         # Fill the black background with white color\n#         # rgb to hsv color space\n#         hsv_img = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n\n#         s_ch = hsv_img[:, :, 1]  # Get the saturation channel\n\n#         # Apply threshold - pixels above 5 are going to be 255, other are zeros.\n#         thesh = cv2.threshold(s_ch, 5, 255, cv2.THRESH_BINARY)[1]\n#         # Apply opening morphological operation for removing artifacts.\n#         thesh = cv2.morphologyEx(\n#             thesh, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7)))\n\n#         # Fill the background in thesh with the value 128 (pixel in the foreground stays 0.\n#         cv2.floodFill(thesh, None, seedPoint=(0, 0),\n#                       newVal=255, loDiff=1, upDiff=1)\n\n#         # Set all the pixels where thesh=128 to red.\n#         image[thesh == 128] = (255, 255, 255)\n\n#         return image\n\n#     def remove_bg2(self, image):\n#         return remove(image)\n\n#     def draw_bounding_box_on_image(self,\n#                                    image,\n#                                    ymin,\n#                                    xmin,\n#                                    ymax,\n#                                    xmax,\n#                                    font,\n#                                    color='#FFFFFF',\n#                                    thickness=1,\n#                                    display_str_list=()):\n#         \"\"\"\n#         Adds a bounding box to an image.\n\n#         Args:\n#             image -- the image object\n#             ymin -- bounding box coordinate\n#             xmin -- bounding box coordinate\n#             ymax -- bounding box coordinate\n#             xmax -- bounding box coordinate\n#             color -- color for the bounding box edges\n#             font -- font for class label\n#             thickness -- edge thickness of the bounding box\n#             display_str_list -- class labels for each object detected\n\n\n#         Returns:\n#             No return.  The function modifies the `image` argument \n#                         that gets passed into this function\n\n#         \"\"\"\n#         draw = ImageDraw.Draw(image)\n#         im_width, im_height = image.size\n\n#         # scale the bounding box coordinates to the height and width of the image\n#         (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n#                                       ymin * im_height, ymax * im_height)\n\n#         # define the four edges of the detection box\n#         draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n#                    (left, top)],\n#                   width=thickness,\n#                   fill=color)\n\n#     def draw_boxes(self, image, boxes, class_names, scores):\n#         \"\"\"\n#         Overlay labeled boxes on an image with formatted scores and label names.\n\n#         Args:\n#             image -- the image as a numpy array\n#             boxes -- list of detection boxes\n#             class_names -- list of classes for each detected object\n#             scores -- numbers showing the model's confidence in detecting that object\n#             max_boxes -- maximum detection boxes to overlay on the image (default is 10)\n#             min_score -- minimum score required to display a bounding box\n\n#         Returns:\n#             image -- the image after detection boxes and classes are overlaid on the original image.\n#         \"\"\"\n#         # colors = list(ImageColor.colormap.values())\n\n#         try:\n#             font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\",\n#                                       25)\n#         except IOError:\n#             #print(\"Font not found, using default font.\")\n#             font = ImageFont.load_default()\n\n#             # only display detection boxes that have the minimum score or higher\n#         idx = np.argmax(scores)\n\n#         ymin, xmin, ymax, xmax = tuple(boxes[idx])\n\n#         # \"{}: {}%\".format(class_names[i].decode(\"ascii\"),int(100 * scores[i]))\n#         display_str = f'Predicted class - {class_names[idx]}'\n\n#         # color = colors[hash(class_names[idx]) % len(colors)]\n#         image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n\n#         # draw one bounding box and overlay the class labels onto the image\n#         self.draw_bounding_box_on_image(image_pil,\n#                                         ymin,\n#                                         xmin,\n#                                         ymax,\n#                                         xmax,\n#                                         # color,\n#                                         font,\n#                                         display_str_list=[display_str])\n#         np.copyto(image, np.array(image_pil))\n\n#         return image\n\n#     def make_border(self, img_array, border_percent=0.03, display_shape=False):\n#         border_type = cv2.BORDER_CONSTANT\n#         img_shape = img_array.shape[:2]\n#         # Initialize arguments for the filter\n#         top = int(border_percent * img_array.shape[0])  # shape[0] = rows\n#         bottom = top\n#         left = int(border_percent * img_array.shape[1])  # shape[1] = cols\n#         right = left\n\n#         img_array = cv2.copyMakeBorder(\n#             img_array, top, bottom, left, right, border_type, None, value=(255, 255, 255))\n#         if display_shape:\n#             print(img_shape)\n#         img_array = cv2.resize(src=img_array, dsize=img_shape)\n\n#         return img_array\n\n#     def sharpen_image(self, img_array, sharpening_kernel):\n#         # sharpening_kernel = np.array([[0, -1, 0],\n#         #                       [-1, 5, -1],\n#         #                       [0, -1, 0]])\n\n#         img_array = cv2.filter2D(\n#             src=img_array, ddepth=-1, kernel=sharpening_kernel)\n#         return img_array\n\n#     def increase_brightness_contrast(self, img_array):\n#         alpha = 1.1  # Simple contrast control\n#         beta = 1.2    # Simple brightness control\n#         final_image = np.zeros(img_array.shape, img_array.dtype)\n#         for y in range(img_array.shape[0]):\n#             for x in range(img_array.shape[1]):\n#                 for ch in range(img_array.shape[2]):\n#                     final_image[y, x, ch] = np.clip(\n#                         alpha*img_array[y, x, ch] + beta, 0, 255)\n\n#         return final_image\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.707438Z","iopub.execute_input":"2023-09-23T17:54:24.708027Z","iopub.status.idle":"2023-09-23T17:54:24.721708Z","shell.execute_reply.started":"2023-09-23T17:54:24.707995Z","shell.execute_reply":"2023-09-23T17:54:24.720744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class LocalizeObjects(HelperFunctions):\n#     def __init__(self, img_dsize=IMG_SIZE_TRAIN) -> None:\n\n#         self._img_dsize = img_dsize\n#         self.__model_ref = None\n#         self._buffer_img_array = None\n#         super().__init__()\n\n#     def extract_model(self, file_path, dir):\n#         model_file = tarfile.open(file_path)\n#         model_file.extractall(dir)\n#         model_file.close()\n\n#     def load_model(self, model_type):\n#         if isinstance(model_type, enum.Enum):\n#             model_type = model_type.value\n\n#         if model_type in OBJECT_DETECTION_MODEL_DICT:\n\n#             model_dir = OBJECT_DETECTION_MODEL_DICT[model_type]['dir']\n#             model_name = OBJECT_DETECTION_MODEL_DICT[model_type]['name']\n#             file_path = os.path.join(model_dir, model_name)\n\n#             if not pathlib.Path(f'{model_dir}/saved_model.pb').exists():\n#                 self.extract_model(file_path, model_dir)\n\n#             self._model_ref = hub.load(model_dir)\n#             self._model_ref = self._model_ref.signatures['default']\n#             print(f'Model loaded from -> {model_dir}')\n#         else:\n#             raise Exception(\n#                 f'{model_type} is not mapped to any object detection model')\n\n#     def read_resize_image(self, image_path, display=False):\n#         try:\n#             # image_path = str(os.path.abspath(image_path))\n#             _, _img_array = self.read_single_image(_img_path=image_path)\n#             _size_1 = _img_array.shape\n#             _img_array = self.resize_image(\n#                 _img_array, new_dsize=self._img_dsize)\n#             self._buffer_img_array = _img_array\n\n#             if display:\n#                 print(f'Old size - {_size_1}, New size - {_img_array.shape}')\n#                 self.display_image(img_array=_img_array, desc=f'Resized image')\n\n#         except Exception as exp:\n#             print(exp)\n\n#     def predict_bbox(self):\n#         _converted_img = tf.image.convert_image_dtype(\n#             self._buffer_img_array, tf.float32)[tf.newaxis, ...]\n#         result = self._model_ref(_converted_img)\n#         result = {key: value.numpy() for key, value in result.items()}\n#         #image_with_boxes = self.draw_boxes(img_array.numpy(), result[\"detection_boxes\"],result[\"detection_class_entities\"], result[\"detection_scores\"])\n#         return result\n\n#     def draw_predicted_bbox(self, result_dict, is_diplay=False):\n#         img_box = self.draw_boxes(self._buffer_img_array,\n#                                   result_dict[\"detection_boxes\"],\n#                                   result_dict[\"detection_class_entities\"],\n#                                   result_dict[\"detection_scores\"])\n#         if is_diplay:\n#             self.display_image(img_box)\n\n#         return img_box\n\n#     def get_object_dimensions(self, img_size, result_dict):\n#         boxes = result_dict[\"detection_boxes\"]\n#         scores = result_dict[\"detection_scores\"]\n#         box = boxes[np.argmax(scores)]\n\n#         ymin, xmin, ymax, xmax = tuple(box)\n#         # idx = np.argmax(scores)\n#         im_width, im_height = self._img_dsize\n\n#         # scale the bounding box coordinates to the height and width of the image\n#         (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n#                                       ymin * im_height, ymax * im_height)\n\n#         return (left, right, top, bottom)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.724644Z","iopub.execute_input":"2023-09-23T17:54:24.724899Z","iopub.status.idle":"2023-09-23T17:54:24.739523Z","shell.execute_reply.started":"2023-09-23T17:54:24.724877Z","shell.execute_reply":"2023-09-23T17:54:24.738382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## Create class objects for image pre-processing ##\n\n# lo = LocalizeObjects()\n# hf = HelperFunctions()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.742305Z","iopub.execute_input":"2023-09-23T17:54:24.742867Z","iopub.status.idle":"2023-09-23T17:54:24.755158Z","shell.execute_reply.started":"2023-09-23T17:54:24.742835Z","shell.execute_reply":"2023-09-23T17:54:24.754002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# lo.load_model(model_type=model_type.inception_resnet_rcnn)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.758286Z","iopub.execute_input":"2023-09-23T17:54:24.758555Z","iopub.status.idle":"2023-09-23T17:54:24.767080Z","shell.execute_reply.started":"2023-09-23T17:54:24.758533Z","shell.execute_reply":"2023-09-23T17:54:24.765842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Experiments with pre-processing pipeline","metadata":{}},{"cell_type":"code","source":"# exp_path, orig_image = hf.read_single_image(\n#     sickness_name='algal leaf', randomly=True)\n\n# hf.display_image(orig_image, desc='original_image')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.768422Z","iopub.execute_input":"2023-09-23T17:54:24.768798Z","iopub.status.idle":"2023-09-23T17:54:24.778312Z","shell.execute_reply.started":"2023-09-23T17:54:24.768766Z","shell.execute_reply":"2023-09-23T17:54:24.777374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(exp_path)\n# lo.read_resize_image(str(exp_path), display=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.779889Z","iopub.execute_input":"2023-09-23T17:54:24.780395Z","iopub.status.idle":"2023-09-23T17:54:24.790154Z","shell.execute_reply.started":"2023-09-23T17:54:24.780363Z","shell.execute_reply":"2023-09-23T17:54:24.789300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# res_dict = lo.predict_bbox()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.791442Z","iopub.execute_input":"2023-09-23T17:54:24.791880Z","iopub.status.idle":"2023-09-23T17:54:24.800939Z","shell.execute_reply.started":"2023-09-23T17:54:24.791848Z","shell.execute_reply":"2023-09-23T17:54:24.799966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# res_dict['detection_class_entities'][np.argmax(res_dict['detection_scores'])]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.804165Z","iopub.execute_input":"2023-09-23T17:54:24.804487Z","iopub.status.idle":"2023-09-23T17:54:24.812549Z","shell.execute_reply.started":"2023-09-23T17:54:24.804463Z","shell.execute_reply":"2023-09-23T17:54:24.811621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %time\n# pred_bbox_img = lo.draw_predicted_bbox(res_dict, is_diplay=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.815590Z","iopub.execute_input":"2023-09-23T17:54:24.815845Z","iopub.status.idle":"2023-09-23T17:54:24.824293Z","shell.execute_reply.started":"2023-09-23T17:54:24.815823Z","shell.execute_reply":"2023-09-23T17:54:24.823280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# left, right, top, bottom = tuple(round(v)\n#                                  for v in lo.get_object_dimensions(0, res_dict))\n# left, right, top, bottom\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.825692Z","iopub.execute_input":"2023-09-23T17:54:24.826004Z","iopub.status.idle":"2023-09-23T17:54:24.835295Z","shell.execute_reply.started":"2023-09-23T17:54:24.825979Z","shell.execute_reply":"2023-09-23T17:54:24.834286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_bbox_img_cropped = pred_bbox_img[top:bottom, left:right]\n# pred_bbox_img_cropped = hf.resize_image(\n#     pred_bbox_img_cropped, new_dsize=IMG_SIZE_TRAIN)\n# hf.display_image(pred_bbox_img_cropped, desc='Cropped image')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.836415Z","iopub.execute_input":"2023-09-23T17:54:24.836876Z","iopub.status.idle":"2023-09-23T17:54:24.851350Z","shell.execute_reply.started":"2023-09-23T17:54:24.836845Z","shell.execute_reply":"2023-09-23T17:54:24.850406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_bbox_img_cropped_border = hf.make_border(\n#     pred_bbox_img_cropped, border_percent=0.01)\n# hf.display_image(pred_bbox_img_cropped_border, desc='image with border')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.853766Z","iopub.execute_input":"2023-09-23T17:54:24.854051Z","iopub.status.idle":"2023-09-23T17:54:24.863741Z","shell.execute_reply.started":"2023-09-23T17:54:24.854027Z","shell.execute_reply":"2023-09-23T17:54:24.862743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_bbox_img_cropped_border_rembg_1 = hf.remove_bg(\n#     pred_bbox_img_cropped_border)\n# hf.display_image(pred_bbox_img_cropped_border_rembg_1,\n#                  desc='BG removed method-1')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.865401Z","iopub.execute_input":"2023-09-23T17:54:24.866109Z","iopub.status.idle":"2023-09-23T17:54:24.875358Z","shell.execute_reply.started":"2023-09-23T17:54:24.866079Z","shell.execute_reply":"2023-09-23T17:54:24.874406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %time\n\n# pred_bbox_img_cropped_border_rembg_2 = hf.remove_bg2(\n#     pred_bbox_img_cropped_border)\n# hf.display_image(pred_bbox_img_cropped_border_rembg_2,\n#                  desc='BG removed method-2')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.876681Z","iopub.execute_input":"2023-09-23T17:54:24.877132Z","iopub.status.idle":"2023-09-23T17:54:24.886277Z","shell.execute_reply.started":"2023-09-23T17:54:24.877101Z","shell.execute_reply":"2023-09-23T17:54:24.885298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sharpening_kernel = np.array([[0, -1, 0],\n#                               [-1, 5, -1],\n#                               [0, -1, 0]])\n\n# image_sharp = hf.sharpen_image(\n#     pred_bbox_img_cropped_border_rembg_2, sharpening_kernel)\n# hf.display_image(image_sharp, desc='Shapened image')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.887557Z","iopub.execute_input":"2023-09-23T17:54:24.888520Z","iopub.status.idle":"2023-09-23T17:54:24.899198Z","shell.execute_reply.started":"2023-09-23T17:54:24.888488Z","shell.execute_reply":"2023-09-23T17:54:24.898186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final_image = hf.increase_brightness_contrast(image_sharp)\n# hf.display_image(final_image)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.900588Z","iopub.execute_input":"2023-09-23T17:54:24.901520Z","iopub.status.idle":"2023-09-23T17:54:24.911173Z","shell.execute_reply.started":"2023-09-23T17:54:24.901441Z","shell.execute_reply":"2023-09-23T17:54:24.910301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def plot_image(ax, img_array, desc, fontsize=14):\n#     ax.imshow(img_array)\n#     ax.tick_params(left=False, right=False, labelleft=False,\n#                    labelbottom=False, bottom=False)\n#     ax.set_title(desc, fontsize=fontsize)\n#     # ax.locator_params(nbins=4)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.914043Z","iopub.execute_input":"2023-09-23T17:54:24.914411Z","iopub.status.idle":"2023-09-23T17:54:24.922917Z","shell.execute_reply.started":"2023-09-23T17:54:24.914384Z","shell.execute_reply":"2023-09-23T17:54:24.921976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, ax = plt.subplots(nrows=2, ncols=4, figsize=[10, 10])\n\n# plot_image(ax[0][0], orig_image, desc='original_image')\n# plot_image(ax[0][1], pred_bbox_img, desc='predicted_bbox')\n# plot_image(ax[0][2], pred_bbox_img_cropped, desc='cropped_image')\n# plot_image(ax[0][3], pred_bbox_img_cropped_border,\n#            desc='cropped_image_with_border')\n# plot_image(ax[1][0], pred_bbox_img_cropped_border_rembg_1,\n#            desc='bg_removed_1')\n# plot_image(ax[1][1], pred_bbox_img_cropped_border_rembg_2,\n#            desc='bg_removed_2')\n\n\n# plot_image(ax[1][2], image_sharp, desc='sharpened_image')\n# plot_image(ax[1][3], final_image, desc='bright_contrast_image')\n# plt.tight_layout(pad=0.1, w_pad=0.1, h_pad=0.25)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.933054Z","iopub.execute_input":"2023-09-23T17:54:24.933714Z","iopub.status.idle":"2023-09-23T17:54:24.938336Z","shell.execute_reply.started":"2023-09-23T17:54:24.933689Z","shell.execute_reply":"2023-09-23T17:54:24.937248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pre-processing pipeline run automated","metadata":{}},{"cell_type":"code","source":"# %time\n# hf = HelperFunctions()\n# lo = LocalizeObjects()\n# lo.load_model(model_type=model_type.inception_resnet_rcnn)\n# sharpening_kernel = np.array([[0, -1, 0],\n#                               [-1, 5, -1],\n#                               [0, -1, 0]])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.939568Z","iopub.execute_input":"2023-09-23T17:54:24.940565Z","iopub.status.idle":"2023-09-23T17:54:24.952348Z","shell.execute_reply.started":"2023-09-23T17:54:24.940534Z","shell.execute_reply":"2023-09-23T17:54:24.951427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######################\n##### Legacy code ####\n######################\n\n\n# for disease, path_lst in sickness_folder_dict.items():\n\n#     tmp_path = path_lst[0]\n#     tmp_dir = os.path.abspath(os.path.dirname(tmp_path))\n#     processed_dir = os.path.join(tmp_dir, 'processed')\n#     if not os.path.exists(processed_dir):\n#         os.mkdir(processed_dir)\n#     else:\n#         shutil.rmtree(processed_dir)\n#         os.mkdir(processed_dir)\n\n#     dt_lst = []\n#     for path in tqdm(path_lst, desc=f'Pre-processing images of {disease}...'):\n\n#         t1 = datetime.datetime.now()\n#         file_name = os.path.basename(path)\n\n#         lo.read_resize_image(path)\n#         res_dict = lo.predict_bbox()\n#         pred_bbox_img = lo.draw_predicted_bbox(res_dict, is_diplay=False)\n\n#         _top, _bottom, _left, _right = tuple(\n#             round(v) for v in lo.get_object_dimensions(0, res_dict))\n#         # _top, _bottom, _left, _right = _top-1, _bottom+1, _left-1, _right+1\n\n#         pred_bbox_img_cropped = pred_bbox_img[_left:_right, _top:_bottom]\n\n#         pred_bbox_img_cropped = hf.resize_image(\n#             pred_bbox_img_cropped, new_dsize=IMG_SIZE_TRAIN)\n\n#         pred_bbox_img_cropped_border = hf.make_border(\n#             pred_bbox_img_cropped, border_percent=0.01)\n\n#         pred_bbox_img_cropped_border_rembg_2 = hf.remove_bg2(\n#             pred_bbox_img_cropped_border)\n\n#         image_sharp = hf.sharpen_image(\n#             pred_bbox_img_cropped_border_rembg_2, sharpening_kernel)\n\n#         final_image = hf.increase_brightness_contrast(image_sharp)\n\n#         file_path = os.path.join(processed_dir, file_name)\n#         cv2.imwrite(file_path, final_image)\n\n#         dt = datetime.datetime.now() - t1\n#         dt_lst.append(dt.seconds+(dt.microseconds*10e-6))\n\n#     print(\n#         f'INFO: Average processing time for each image of {disease} is {sum(dt_lst)/len(dt_lst)} seconds')\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-09-23T17:54:24.954121Z","iopub.execute_input":"2023-09-23T17:54:24.954511Z","iopub.status.idle":"2023-09-23T17:54:24.969190Z","shell.execute_reply.started":"2023-09-23T17:54:24.954479Z","shell.execute_reply":"2023-09-23T17:54:24.968242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# def run_image_preprocessing():\n#     hf = HelperFunctions()\n#     lo = LocalizeObjects()\n#     lo.load_model(model_type=model_type.inception_resnet_rcnn)\n#     sharpening_kernel = np.array([[0, -1, 0],\n#                                   [-1, 5, -1],\n#                                   [0, -1, 0]])\n\n#     for disease, path_lst in sickness_folder_dict.items():\n\n#         tmp_path = path_lst[0]\n#         tmp_dir = os.path.abspath(os.path.dirname(tmp_path))\n#         processed_dir = os.path.join(tmp_dir, 'processed')\n#         if not os.path.exists(processed_dir):\n#             os.mkdir(processed_dir)\n#         else:\n#             shutil.rmtree(processed_dir)\n#             os.mkdir(processed_dir)\n\n#         dt_lst = []\n#         for path in tqdm(path_lst, desc=f'Pre-processing images of {disease}...'):\n\n#             t1 = datetime.datetime.now()\n#             file_name = os.path.basename(path)\n\n#             lo.read_resize_image(path)\n#             res_dict = lo.predict_bbox()\n#             pred_bbox_img = lo.draw_predicted_bbox(res_dict, is_diplay=False)\n\n#             _top, _bottom, _left, _right = tuple(\n#                 round(v) for v in lo.get_object_dimensions(0, res_dict))\n#             # _top, _bottom, _left, _right = _top-1, _bottom+1, _left-1, _right+1\n\n#             pred_bbox_img_cropped = pred_bbox_img[_left:_right, _top:_bottom]\n\n#             pred_bbox_img_cropped = hf.resize_image(\n#                 pred_bbox_img_cropped, new_dsize=IMG_SIZE_TRAIN)\n\n#             pred_bbox_img_cropped_border = hf.make_border(\n#                 pred_bbox_img_cropped, border_percent=0.01)\n\n#             pred_bbox_img_cropped_border_rembg_2 = hf.remove_bg2(\n#                 pred_bbox_img_cropped_border)\n\n#             image_sharp = hf.sharpen_image(\n#                 pred_bbox_img_cropped_border_rembg_2, sharpening_kernel)\n\n#             final_image = hf.increase_brightness_contrast(image_sharp)\n\n#             file_path = os.path.join(processed_dir, file_name)\n#             cv2.imwrite(file_path, final_image)\n\n#             dt = datetime.datetime.now() - t1\n#             dt_lst.append(dt.seconds+(dt.microseconds*10e-6))\n\n#         print(\n#             f'INFO: Average processing time for each image of {disease} is {sum(dt_lst)/len(dt_lst)} seconds')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.970693Z","iopub.execute_input":"2023-09-23T17:54:24.971019Z","iopub.status.idle":"2023-09-23T17:54:24.985934Z","shell.execute_reply.started":"2023-09-23T17:54:24.970988Z","shell.execute_reply":"2023-09-23T17:54:24.984989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %time\n# run_image_preprocessing()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:24.987502Z","iopub.execute_input":"2023-09-23T17:54:24.988117Z","iopub.status.idle":"2023-09-23T17:54:25.000483Z","shell.execute_reply.started":"2023-09-23T17:54:24.988086Z","shell.execute_reply":"2023-09-23T17:54:24.999629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Preprocessed Dataset loading pipeline","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Load the preprocessed data","metadata":{}},{"cell_type":"code","source":"def get_preprocessed_fp_dict(old_sickness_folder_dict):\n    sickness_folder_processed_dict = defaultdict()\n    for key, values in old_sickness_folder_dict.items():\n        dir_name = os.path.dirname(values[0])\n        new_path_lst = []\n        for val in values:\n\n            new_path = os.path.join(\n                dir_name, 'processed', os.path.basename(val))\n            new_path_lst.append(pb.Path(new_path))\n\n        sickness_folder_processed_dict.update({key: new_path_lst})\n\n    return sickness_folder_processed_dict\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.002031Z","iopub.execute_input":"2023-09-23T17:54:25.002711Z","iopub.status.idle":"2023-09-23T17:54:25.012662Z","shell.execute_reply.started":"2023-09-23T17:54:25.002678Z","shell.execute_reply":"2023-09-23T17:54:25.011704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sickness_folder_processed_dict = get_preprocessed_fp_dict(sickness_folder_dict)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.014084Z","iopub.execute_input":"2023-09-23T17:54:25.014801Z","iopub.status.idle":"2023-09-23T17:54:25.035406Z","shell.execute_reply.started":"2023-09-23T17:54:25.014770Z","shell.execute_reply":"2023-09-23T17:54:25.034487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Split the image data paths into training and testing ","metadata":{}},{"cell_type":"code","source":"\ndef get_train_test_fp_list(processed_sickness_folder_dict, train_split_ratio=0.8, test_split_ratio=0.2):\n    train_img_path_lst = []\n    test_img_path_lst = []\n\n    for _, values in sickness_folder_processed_dict.items():\n        number_of_images = len(values)\n        values_lst = list(values)\n        number_of_train_images = int(\n            np.ceil(number_of_images*train_split_ratio))\n        number_of_test_images = int(\n            np.floor(number_of_images*test_split_ratio))\n\n        train_image_paths = np.random.choice(\n            values_lst, number_of_train_images)\n        test_image_paths = np.random.choice(values_lst, number_of_test_images)\n\n        train_img_path_lst.extend(train_image_paths)\n        test_img_path_lst.extend(test_image_paths)\n\n    return (train_img_path_lst, test_img_path_lst)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.037009Z","iopub.execute_input":"2023-09-23T17:54:25.037666Z","iopub.status.idle":"2023-09-23T17:54:25.047302Z","shell.execute_reply.started":"2023-09-23T17:54:25.037633Z","shell.execute_reply":"2023-09-23T17:54:25.046416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_path_lst, test_img_path_lst = get_train_test_fp_list(\n    sickness_folder_processed_dict)\n\nprint(\n    f'Train images # -> {len(train_img_path_lst)}\\nTest images # -> {len(test_img_path_lst)}')\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.050551Z","iopub.execute_input":"2023-09-23T17:54:25.050810Z","iopub.status.idle":"2023-09-23T17:54:25.066431Z","shell.execute_reply.started":"2023-09-23T17:54:25.050787Z","shell.execute_reply":"2023-09-23T17:54:25.065298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ### TEST ###\n# a = np.array(range(10), dtype=np.float32).reshape(-1,1)\n# b = np.array(range(10), dtype=np.uint8).reshape(-1,1)\n\n# tf.concat([a,b], axis = 1)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.067756Z","iopub.execute_input":"2023-09-23T17:54:25.068412Z","iopub.status.idle":"2023-09-23T17:54:25.077775Z","shell.execute_reply.started":"2023-09-23T17:54:25.068327Z","shell.execute_reply":"2023-09-23T17:54:25.076696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_img_path_and_labels_df(img_path_lst, parents_idx=1) -> pd.DataFrame:\n    \"\"\"Generate a pandas dataframe having the image paths, disease and OHE vectors as columns\"\"\"\n    df = pd.DataFrame(img_path_lst, columns=['paths'])\n    df['paths'] = df['paths'].apply(lambda x: str(os.path.abspath(x)))\n\n    df['disease'] = df['paths'].apply(\n        lambda x: os.path.basename(list(pb.Path(x).parents)[parents_idx]))\n\n    disease_df = pd.get_dummies(df['disease'])  # , dtype = np.float16)\n    df = pd.concat([df, disease_df], axis=1)\n    df.columns = df.columns.str.lower()\n    df['disease'] = df['disease'].str.lower()\n    return df\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.079330Z","iopub.execute_input":"2023-09-23T17:54:25.079731Z","iopub.status.idle":"2023-09-23T17:54:25.094374Z","shell.execute_reply.started":"2023-09-23T17:54:25.079700Z","shell.execute_reply":"2023-09-23T17:54:25.093234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_df = get_img_path_and_labels_df(train_img_path_lst, parents_idx=1)\ntrain_data_df  # .to_clipboard()\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.095831Z","iopub.execute_input":"2023-09-23T17:54:25.096157Z","iopub.status.idle":"2023-09-23T17:54:25.157585Z","shell.execute_reply.started":"2023-09-23T17:54:25.096127Z","shell.execute_reply":"2023-09-23T17:54:25.156669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_df['disease'].unique()\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.158828Z","iopub.execute_input":"2023-09-23T17:54:25.159207Z","iopub.status.idle":"2023-09-23T17:54:25.166326Z","shell.execute_reply.started":"2023-09-23T17:54:25.159173Z","shell.execute_reply":"2023-09-23T17:54:25.165450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual_data_df = train_data_df.copy()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:25.167755Z","iopub.execute_input":"2023-09-23T17:54:25.168358Z","iopub.status.idle":"2023-09-23T17:54:25.177212Z","shell.execute_reply.started":"2023-09-23T17:54:25.168327Z","shell.execute_reply":"2023-09-23T17:54:25.176297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<br>\n<br>\n<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"### 4.3 Geometrical Augmentation\n\n","metadata":{}},{"cell_type":"code","source":"def geometrical_augmentation(tmp_src_aug_dir, dst_aug_dir, number_of_image_to_augment):\n    p = Augmentor.Pipeline(tmp_src_aug_dir, output_directory=dst_aug_dir)\n\n    p.flip_random(probability=0.5)\n\n    # Add a shear operation to the pipeline\n    p.shear(probability=0.2, max_shear_left=0.3, max_shear_right=0.3)\n\n    # Add a rotate operation to the pipeline:\n    p.rotate_random_90(probability=0.5)\n\n    p.skew_corner(probability=0.3)\n\n    # We are adding such that total # of  samples per class is same i.e. 1000, to make sure that none\n    p.sample(number_of_image_to_augment)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:25.178846Z","iopub.execute_input":"2023-09-23T17:54:25.179494Z","iopub.status.idle":"2023-09-23T17:54:25.189434Z","shell.execute_reply.started":"2023-09-23T17:54:25.179443Z","shell.execute_reply":"2023-09-23T17:54:25.188531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def handle_augmentation(train_data_df, augment_split_ratio=0.6):\n    aug_count = 0\n    for disease in tqdm(train_data_df['disease'].unique(), desc='Augmenting images...'):\n        df_path_disease_lst = train_data_df[train_data_df['disease']\n                                            == disease]['paths'].values.tolist()\n        number_of_image_to_augment = int(\n            np.floor(len(df_path_disease_lst)*augment_split_ratio))\n        print(\n            f'For {disease}, number of images to augment is {number_of_image_to_augment}')\n\n        # processed dir path of a sickness folder\n        dir_path = os.path.dirname(df_path_disease_lst[0])\n\n        # temporary augmentation directory for storing buffer data\n        tmp_src_aug_dir = os.path.join(dir_path, 'tmp_aug')\n        if os.path.exists(tmp_src_aug_dir):\n            shutil.rmtree(tmp_src_aug_dir)\n        else:\n            os.mkdir(tmp_src_aug_dir)\n\n        # copying the train images of a disease into the temporary directory\n        for img_path in df_path_disease_lst:\n            tmp_path = os.path.join(\n                tmp_src_aug_dir, os.path.basename(img_path))\n            shutil.copy(img_path, tmp_path)\n\n        # augmentation destination directory\n        dst_aug_dir = os.path.join(dir_path, 'output')\n\n        # deleting the previous augmentation data from the destibnation folder\n        if os.path.exists(dst_aug_dir):\n            shutil.rmtree(dst_aug_dir)\n\n        geometrical_augmentation(\n            tmp_src_aug_dir, dst_aug_dir, number_of_image_to_augment)\n\n        # deleting the temporary augmentation source directory after usage\n        if os.path.exists(tmp_src_aug_dir):\n            shutil.rmtree(tmp_src_aug_dir)\n\n        aug_count += number_of_image_to_augment\n\n    print(f'Number of images augmented is {aug_count}')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:25.191129Z","iopub.execute_input":"2023-09-23T17:54:25.191850Z","iopub.status.idle":"2023-09-23T17:54:25.203028Z","shell.execute_reply.started":"2023-09-23T17:54:25.191816Z","shell.execute_reply":"2023-09-23T17:54:25.202305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Run the image augmentation\n\n# handle_augmentation(train_data_df)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:25.204543Z","iopub.execute_input":"2023-09-23T17:54:25.205126Z","iopub.status.idle":"2023-09-23T17:54:25.218951Z","shell.execute_reply.started":"2023-09-23T17:54:25.205094Z","shell.execute_reply":"2023-09-23T17:54:25.217986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.4 Load the baseline model dataset","metadata":{}},{"cell_type":"markdown","source":"#### 4.4.1 Load all the augmented image paths","metadata":{}},{"cell_type":"code","source":"def get_augmented_image_list(train_data_df):\n    tmp_train_data_df = train_data_df.copy()\n    aug_img_list_all_disease = []\n    for disease in train_data_df['disease'].unique():\n        tmp_df = pd.DataFrame()\n        df_path_disease_lst = train_data_df[train_data_df['disease']\n                                            == disease]['paths'].values.tolist()\n        dir_path = os.path.dirname(df_path_disease_lst[0])\n        # print(dir_path)\n        aug_dir_path = os.path.join(dir_path, 'output')\n        aug_img_list = list(pb.Path(aug_dir_path).glob('*.jpg'))\n\n        if len(aug_img_list) > 0:\n            aug_img_list_all_disease.extend(aug_img_list)\n        else:\n            raise Exception(\n                f'No augmentation output folder found in {os.path.dirname(aug_dir_path)}')\n    return aug_img_list_all_disease\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.220435Z","iopub.execute_input":"2023-09-23T17:54:25.221081Z","iopub.status.idle":"2023-09-23T17:54:25.231335Z","shell.execute_reply.started":"2023-09-23T17:54:25.221049Z","shell.execute_reply":"2023-09-23T17:54:25.230247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aug_img_list_all_disease = get_augmented_image_list(train_data_df)\nlen(aug_img_list_all_disease)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.232801Z","iopub.execute_input":"2023-09-23T17:54:25.233522Z","iopub.status.idle":"2023-09-23T17:54:25.463156Z","shell.execute_reply.started":"2023-09-23T17:54:25.233488Z","shell.execute_reply":"2023-09-23T17:54:25.462297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augmented_df = get_img_path_and_labels_df(\n    aug_img_list_all_disease, parents_idx=2)\naugmented_df\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.465211Z","iopub.execute_input":"2023-09-23T17:54:25.465801Z","iopub.status.idle":"2023-09-23T17:54:25.506379Z","shell.execute_reply.started":"2023-09-23T17:54:25.465768Z","shell.execute_reply":"2023-09-23T17:54:25.505294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.4.2 Concatenate the real and geometrically augmented image paths","metadata":{}},{"cell_type":"code","source":"print(f'Before augmentation - {train_data_df.shape}')\ntrain_data_df = pd.concat([train_data_df, augmented_df], axis=0)\nprint(f'After augmentation - {train_data_df.shape}')\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.508037Z","iopub.execute_input":"2023-09-23T17:54:25.508422Z","iopub.status.idle":"2023-09-23T17:54:25.516439Z","shell.execute_reply.started":"2023-09-23T17:54:25.508388Z","shell.execute_reply":"2023-09-23T17:54:25.515476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_df\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.517670Z","iopub.execute_input":"2023-09-23T17:54:25.518806Z","iopub.status.idle":"2023-09-23T17:54:25.544897Z","shell.execute_reply.started":"2023-09-23T17:54:25.518758Z","shell.execute_reply":"2023-09-23T17:54:25.543763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_df.disease.value_counts().plot.bar(color='y')\nplt.show()\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.547245Z","iopub.execute_input":"2023-09-23T17:54:25.548399Z","iopub.status.idle":"2023-09-23T17:54:25.862291Z","shell.execute_reply.started":"2023-09-23T17:54:25.548366Z","shell.execute_reply":"2023-09-23T17:54:25.861203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.4.3 Getting X-image paths and y-label one-hot encoded vectors","metadata":{}},{"cell_type":"code","source":"paths_X_train = train_data_df['paths'].values.tolist()\n\n[paths_X_train[i] for i in np.random.randint(\n    low=0, high=len(paths_X_train)-1, size=5)]\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.864281Z","iopub.execute_input":"2023-09-23T17:54:25.864962Z","iopub.status.idle":"2023-09-23T17:54:25.873173Z","shell.execute_reply.started":"2023-09-23T17:54:25.864927Z","shell.execute_reply":"2023-09-23T17:54:25.872214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lables_y_train = train_data_df[train_data_df['disease'].unique()].values\n\n[lables_y_train[i] for i in np.random.randint(\n    low=0, high=len(lables_y_train)-1, size=5)]\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.875080Z","iopub.execute_input":"2023-09-23T17:54:25.875560Z","iopub.status.idle":"2023-09-23T17:54:25.889213Z","shell.execute_reply.started":"2023-09-23T17:54:25.875527Z","shell.execute_reply":"2023-09-23T17:54:25.888090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_df = get_img_path_and_labels_df(test_img_path_lst, parents_idx=1)\ntest_data_df\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.890537Z","iopub.execute_input":"2023-09-23T17:54:25.891069Z","iopub.status.idle":"2023-09-23T17:54:25.924282Z","shell.execute_reply.started":"2023-09-23T17:54:25.891036Z","shell.execute_reply":"2023-09-23T17:54:25.923195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths_X_test = test_data_df['paths'].values.tolist()\n\n\n[paths_X_test[i] for i in np.random.randint(\n    low=0, high=len(paths_X_test)-1, size=5)]\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.925649Z","iopub.execute_input":"2023-09-23T17:54:25.925945Z","iopub.status.idle":"2023-09-23T17:54:25.934389Z","shell.execute_reply.started":"2023-09-23T17:54:25.925915Z","shell.execute_reply":"2023-09-23T17:54:25.933211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lables_y_test = test_data_df[test_data_df['disease'].unique()].values\n\n[lables_y_test[i] for i in np.random.randint(\n    low=0, high=len(lables_y_test)-1, size=5)]\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.935824Z","iopub.execute_input":"2023-09-23T17:54:25.936657Z","iopub.status.idle":"2023-09-23T17:54:25.949970Z","shell.execute_reply.started":"2023-09-23T17:54:25.936624Z","shell.execute_reply":"2023-09-23T17:54:25.948781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.4.4 Splitting the test set into test set and validation set","metadata":{}},{"cell_type":"code","source":"print(len(paths_X_test))\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.951665Z","iopub.execute_input":"2023-09-23T17:54:25.952363Z","iopub.status.idle":"2023-09-23T17:54:25.964597Z","shell.execute_reply.started":"2023-09-23T17:54:25.952326Z","shell.execute_reply":"2023-09-23T17:54:25.963316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths_X_test, paths_X_val, lables_y_test, lables_y_val = train_test_split(\n    paths_X_test, lables_y_test, train_size=0.6, test_size=0.4, random_state=42)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.966426Z","iopub.execute_input":"2023-09-23T17:54:25.966937Z","iopub.status.idle":"2023-09-23T17:54:25.977504Z","shell.execute_reply.started":"2023-09-23T17:54:25.966810Z","shell.execute_reply":"2023-09-23T17:54:25.976380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(paths_X_test), lables_y_val.shape\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.978790Z","iopub.execute_input":"2023-09-23T17:54:25.979449Z","iopub.status.idle":"2023-09-23T17:54:25.991503Z","shell.execute_reply.started":"2023-09-23T17:54:25.979412Z","shell.execute_reply":"2023-09-23T17:54:25.990369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5 Image dataset streamimg","metadata":{}},{"cell_type":"code","source":"def map_func(image_path, lables):\n    \"\"\" This function will take the image_path & caption and return it's feature & respective caption. \"\"\"\n\n    img_tensor = skimage.io.imread(image_path.decode('utf-8'))\n    img_tensor = img_tensor.astype(np.float32)\n    lables = lables.astype(np.float32)\n\n    return img_tensor, lables\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:25.993209Z","iopub.execute_input":"2023-09-23T17:54:25.994025Z","iopub.status.idle":"2023-09-23T17:54:26.002796Z","shell.execute_reply.started":"2023-09-23T17:54:25.993986Z","shell.execute_reply":"2023-09-23T17:54:26.001580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gen_dataset(img_path, lables, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE_DATASET):\n    \"\"\" Dataset generator function enabling shuffling and parallel CPU core execution \"\"\"\n\n    # Load up the sliced image path and respective captions\n    dataset = tf.data.Dataset.from_tensor_slices((img_path, lables))\n\n    # Maps on top the sliced image and label data\n    # and apply the map_func by using auto-tuning of CPU cores\n    dataset = dataset.map(lambda path, label: tf.numpy_function(map_func, [path, label], [tf.float32, tf.float32]),\n                          num_parallel_calls=tf.data.experimental.AUTOTUNE,)  # name='map_function')\n\n    # Shuffle the data of size equal to batch size and with a prefetch\n    # buffer memory of size 1000 data point using the auto-tuning of CPU cores.\n    dataset = (dataset.shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)).batch(\n        batch_size=batch_size, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n\n    return dataset\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:26.004690Z","iopub.execute_input":"2023-09-23T17:54:26.005525Z","iopub.status.idle":"2023-09-23T17:54:26.016338Z","shell.execute_reply.started":"2023-09-23T17:54:26.005489Z","shell.execute_reply":"2023-09-23T17:54:26.015306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dataset = gen_dataset(paths_X_train, lables_y_train)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:26.019440Z","iopub.execute_input":"2023-09-23T17:54:26.020293Z","iopub.status.idle":"2023-09-23T17:54:26.027174Z","shell.execute_reply.started":"2023-09-23T17:54:26.020226Z","shell.execute_reply":"2023-09-23T17:54:26.026335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_datset = gen_dataset(paths_X_val, lables_y_val)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:26.028792Z","iopub.execute_input":"2023-09-23T17:54:26.029491Z","iopub.status.idle":"2023-09-23T17:54:26.038593Z","shell.execute_reply.started":"2023-09-23T17:54:26.029460Z","shell.execute_reply":"2023-09-23T17:54:26.037837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dataset = gen_dataset(paths_X_test, lables_y_test)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:26.039943Z","iopub.execute_input":"2023-09-23T17:54:26.040658Z","iopub.status.idle":"2023-09-23T17:54:26.049605Z","shell.execute_reply.started":"2023-09-23T17:54:26.040621Z","shell.execute_reply":"2023-09-23T17:54:26.048518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_img_batch, sample_label_batch = next(iter(train_dataset))\n# print(sample_img_batch.shape)  # (batch_size, 8*8, 2048)\n# print(sample_label_batch.shape)  # (batch_size,max_len)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:26.051216Z","iopub.execute_input":"2023-09-23T17:54:26.051927Z","iopub.status.idle":"2023-09-23T17:54:26.060476Z","shell.execute_reply.started":"2023-09-23T17:54:26.051894Z","shell.execute_reply":"2023-09-23T17:54:26.059500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# def get_epoch_settings(paths_X_train, paths_X_val, batch_size=BATCH_SIZE_GAN_TRAIN):\n#     num_train_sequences = len(paths_X_train)\n#     num_val_sequences = len(paths_X_val)\n\n#     if (num_train_sequences % batch_size) == 0:\n\n#         steps_per_epoch = int(num_train_sequences/batch_size)\n#         print(\n#             f'data size is factor of batch size({batch_size}), thus step/epoch = {steps_per_epoch}')\n#     else:\n\n#         steps_per_epoch = (num_train_sequences//batch_size) + 1\n#         print(\n#             f'data size is not factor of batch size({batch_size}), thus step/epoch = {steps_per_epoch}')\n\n#     if (num_val_sequences % batch_size) == 0:\n#         validation_steps = int(num_val_sequences/batch_size)\n#     else:\n#         validation_steps = (num_val_sequences//batch_size) + 1\n\n#     return steps_per_epoch, validation_steps\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.061957Z","iopub.execute_input":"2023-09-23T17:54:26.062721Z","iopub.status.idle":"2023-09-23T17:54:26.076233Z","shell.execute_reply.started":"2023-09-23T17:54:26.062688Z","shell.execute_reply":"2023-09-23T17:54:26.075302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Baseline classification model","metadata":{}},{"cell_type":"code","source":"# steps_per_epoch, validation_steps = get_epoch_settings(\n#     paths_X_train, paths_X_val, batch_size=BATCH_SIZE_TRAIN)\n# steps_per_epoch, validation_steps\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.077629Z","iopub.execute_input":"2023-09-23T17:54:26.078211Z","iopub.status.idle":"2023-09-23T17:54:26.087175Z","shell.execute_reply.started":"2023-09-23T17:54:26.078178Z","shell.execute_reply":"2023-09-23T17:54:26.086129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5.1 Baseline model building","metadata":{}},{"cell_type":"code","source":"# ### Building model through TF sub-classing ####\n\n# class DenseConnection(tf.keras.Model):\n#     def __init__(self, kernels, activations, name) -> None:\n#         super(DenseConnection, self).__init__()\n#         self.dense = tf.keras.layers.Dense(\n#             kernels, activation=activations, name=name)\n\n#     def call(self, input):\n#         x = self.dense(input)\n#         return x\n\n\n# class DenseConnectionWithDropout(DenseConnection):\n#     def __init__(self, kernels, activations, name, dropout) -> None:\n#         super().__init__(kernels, activations, name)\n#         self.dropout = tf.keras.layers.Dropout(dropout)\n\n#     def call(self, input):\n#         x = self.dense(input)\n#         x = self.dropout(x)\n#         return x\n\n\n# class ModelBuilder(tf.keras.Model):\n#     def __init__(self, pretrained_model_ref, num_classes) -> None:\n#         super(ModelBuilder, self).__init__()\n#         self.pretrained_model_ref = pretrained_model_ref\n#         self.global_pool = tf.keras.layers.GlobalMaxPooling2D()\n#         self.flatten = tf.keras.layers.Flatten()\n#         self.dense_1 = DenseConnection(64, activations='relu', name='dense_1')\n#         self.dense_2 = DenseConnectionWithDropout(\n#             64, activations='relu', dropout=0.3, name='dense_2')\n#         self.dense_3 = DenseConnection(32, activations='relu', name='dense_3')\n#         self.dense_4 = DenseConnectionWithDropout(\n#             32, activations='relu', dropout=0.2, name='dense_4')\n#         self.dense_5 = DenseConnectionWithDropout(\n#             16, activations='relu', dropout=0.2, name='dense_5')\n#         self.class_output = tf.keras.layers.Dense(\n#             num_classes, activation='softmax', name='output')\n\n#     def call(self, input):\n#         for layer in self.pretrained_model_ref.layers:\n#             layer.trainable = False\n\n#         x = self.pretrained_model_ref(input)\n#         x = self.global_pool(x)\n#         x = self.flatten(x)\n#         x = self.dense_1(x)\n#         x = self.dense_2(x)\n#         x = self.dense_3(x)\n#         x = self.dense_4(x)\n#         x = self.dense_5(x)\n#         x = self.class_output(x)\n#         return x\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.088877Z","iopub.execute_input":"2023-09-23T17:54:26.089538Z","iopub.status.idle":"2023-09-23T17:54:26.099301Z","shell.execute_reply.started":"2023-09-23T17:54:26.089506Z","shell.execute_reply":"2023-09-23T17:54:26.098370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class ModelHelper():\n#     def __init__(self, model, LR):\n#         self.model = model\n#         self.adam = tf.keras.optimizers.Adam(learning_rate=LR)\n#         self.sgd = tf.keras.optimizers.SGD(learning_rate=LR)\n#         self.rms_prop = tf.keras.optimizers.RMSprop(learning_rate=LR)\n\n#     def compile_model(self, optimizer_name):\n#         optimzer = self.__getattribute__(optimizer_name)\n#         self.model.compile(optimizer=optimzer,\n#                            loss=tf.keras.losses.categorical_crossentropy,\n#                            metrics=['categorical_accuracy'])\n\n#         return self.model\n\n#     def decrease_lr_on_plateau_callback(self, **kwargs):\n#         return tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n#                                                     factor=0.05,\n#                                                     patience=10,\n#                                                     min_delta=0.01,\n#                                                     min_lr=1e-6,\n#                                                     verbose=1)\n\n#     def model_checkpoints_callback(self, filepath, monitor: str = \"val_loss\", mode: str = \"auto\", save_freq=\"epoch\", save_best_only=True):\n#         return tf.keras.callbacks.ModelCheckpoint(filepath,\n#                                                   monitor=monitor,\n#                                                   mode=mode,\n#                                                   save_freq=save_freq,\n#                                                   save_best_only=save_best_only)\n\n#     def plot_model_train_info(self, model_history):\n#         pd.DataFrame(model_history.history).plot(figsize=(8, 5))\n#         plt.grid(True)\n#         plt.title('Model train information', fontsize=14)\n#         plt.xlabel('epoch #')\n#         plt.show()\n\n#     def get_model_info(self, model_history, col_info):\n#         history_df = pd.DataFrame(model_history.history)\n#         loss_min = history_df['loss'].min()\n#         val_loss_min = history_df['val_loss'].min()\n#         categorical_accuracy_max = history_df['categorical_accuracy'].max()\n#         val_categorical_accuracy_max = history_df['val_categorical_accuracy'].max(\n#         )\n\n#         return pd.DataFrame([loss_min, val_loss_min, categorical_accuracy_max, val_categorical_accuracy_max],\n#                             index=['loss_min', 'val_loss_min',\n#                                    'categorical_accuracy_max', 'val_categorical_accuracy_max'],\n#                             columns=[col_info])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.101072Z","iopub.execute_input":"2023-09-23T17:54:26.101716Z","iopub.status.idle":"2023-09-23T17:54:26.116993Z","shell.execute_reply.started":"2023-09-23T17:54:26.101684Z","shell.execute_reply":"2023-09-23T17:54:26.115889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def dump_model(model, directory, info):\n#     model.save_weights(f'{directory}/{info}.h5')\n\n\n# def get_metrics(y_true, y_pred, name):\n#     metrics_dict = {}\n\n#     recall = tf.keras.metrics.Recall()\n#     recall.update_state(y_true=y_true, y_pred=y_pred)\n#     metrics_dict['recall'] = recall.result().numpy()\n\n#     precision = tf.keras.metrics.Precision()\n#     precision.update_state(y_true=y_true, y_pred=y_pred)\n#     metrics_dict['precision'] = precision.result().numpy()\n\n#     auc = tf.keras.metrics.AUC()\n#     auc.update_state(y_true=y_true, y_pred=y_pred)\n#     metrics_dict['auc'] = auc.result().numpy()\n\n#     metrics = pd.DataFrame(data=metrics_dict.values(),\n#                            index=metrics_dict.keys(), columns=[name])\n\n#     return metrics\n\n\n# def get_metrics_for_model(model, model_name):\n#     t_o = time.perf_counter()\n#     y_true = []\n#     data = []\n\n#     for d, v in test_dataset:\n#         y_true.extend(v.numpy())\n#         data.extend(d.numpy())\n\n#     y_true = np.array(y_true)\n#     data = np.array(data)\n#     y_pred = model.predict(data)\n#     metrics_df = get_metrics(y_true, y_pred, model_name)\n#     print(time.perf_counter() - t_o)\n#     return metrics_df\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.118450Z","iopub.execute_input":"2023-09-23T17:54:26.119116Z","iopub.status.idle":"2023-09-23T17:54:26.134614Z","shell.execute_reply.started":"2023-09-23T17:54:26.119083Z","shell.execute_reply":"2023-09-23T17:54:26.133495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Densenet","metadata":{}},{"cell_type":"code","source":"# INPUT_SHAPE = (180, 180, 3)\n# INPUT_SHAPE_BUILD = (None, 180, 180, 3)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.136120Z","iopub.execute_input":"2023-09-23T17:54:26.136878Z","iopub.status.idle":"2023-09-23T17:54:26.150353Z","shell.execute_reply.started":"2023-09-23T17:54:26.136844Z","shell.execute_reply":"2023-09-23T17:54:26.149303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# densenet = DenseNet121(\n#     include_top=False, weights='imagenet', input_shape=INPUT_SHAPE)\n# densenet_model = ModelBuilder(pretrained_model_ref=densenet, num_classes=8)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.151840Z","iopub.execute_input":"2023-09-23T17:54:26.152523Z","iopub.status.idle":"2023-09-23T17:54:26.162647Z","shell.execute_reply.started":"2023-09-23T17:54:26.152489Z","shell.execute_reply":"2023-09-23T17:54:26.161588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_helper = ModelHelper(model=densenet_model, LR=0.0001)\n# densenet_model = model_helper.compile_model(optimizer_name='adam')\n# densenet_model.build(input_shape=INPUT_SHAPE_BUILD)\n# densenet_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.164299Z","iopub.execute_input":"2023-09-23T17:54:26.164983Z","iopub.status.idle":"2023-09-23T17:54:26.175161Z","shell.execute_reply.started":"2023-09-23T17:54:26.164952Z","shell.execute_reply":"2023-09-23T17:54:26.174227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# callbacks_lst = [model_helper.decrease_lr_on_plateau_callback()]\n# densenet_model_history = densenet_model.fit(train_dataset,\n#                                             steps_per_epoch=steps_per_epoch,\n#                                             epochs=100,\n#                                             verbose=1,\n#                                             callbacks=callbacks_lst,\n#                                             validation_data=val_datset,\n#                                             validation_steps=validation_steps,\n#                                             class_weight=None,\n#                                             workers=1,\n#                                             initial_epoch=0)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.176672Z","iopub.execute_input":"2023-09-23T17:54:26.177299Z","iopub.status.idle":"2023-09-23T17:54:26.187339Z","shell.execute_reply.started":"2023-09-23T17:54:26.177244Z","shell.execute_reply":"2023-09-23T17:54:26.186179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_helper.plot_model_train_info(densenet_model_history)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.188788Z","iopub.execute_input":"2023-09-23T17:54:26.189432Z","iopub.status.idle":"2023-09-23T17:54:26.202306Z","shell.execute_reply.started":"2023-09-23T17:54:26.189401Z","shell.execute_reply":"2023-09-23T17:54:26.201265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# densenet_info = model_helper.get_model_info(densenet_model_history, 'densenet')\n# densenet_info\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.203898Z","iopub.execute_input":"2023-09-23T17:54:26.204582Z","iopub.status.idle":"2023-09-23T17:54:26.214202Z","shell.execute_reply.started":"2023-09-23T17:54:26.204550Z","shell.execute_reply":"2023-09-23T17:54:26.213284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# densenet_model.evaluate(test_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.215868Z","iopub.execute_input":"2023-09-23T17:54:26.216584Z","iopub.status.idle":"2023-09-23T17:54:26.227086Z","shell.execute_reply.started":"2023-09-23T17:54:26.216552Z","shell.execute_reply":"2023-09-23T17:54:26.226007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# densenet_metrics = get_metrics_for_model(\n#     model=densenet_model, model_name='densenet')\n# densenet_metrics\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.230051Z","iopub.execute_input":"2023-09-23T17:54:26.230928Z","iopub.status.idle":"2023-09-23T17:54:26.239033Z","shell.execute_reply.started":"2023-09-23T17:54:26.230897Z","shell.execute_reply":"2023-09-23T17:54:26.237989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #\n# dump_model(densenet_model, 'G:/Learning/Degree Courses/MS AI ML/Research/Tea Sickeness Project/Code/output',\n#            'densenet_model_with_aug')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.240369Z","iopub.execute_input":"2023-09-23T17:54:26.240840Z","iopub.status.idle":"2023-09-23T17:54:26.251427Z","shell.execute_reply.started":"2023-09-23T17:54:26.240809Z","shell.execute_reply":"2023-09-23T17:54:26.250402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3 ResNet-50","metadata":{}},{"cell_type":"code","source":"# resnet = resnet50.ResNet50(\n#     include_top=False, weights='imagenet', input_shape=INPUT_SHAPE)\n# resnet_model = ModelBuilder(pretrained_model_ref=resnet, num_classes=8)\n# # densenet_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.252968Z","iopub.execute_input":"2023-09-23T17:54:26.253336Z","iopub.status.idle":"2023-09-23T17:54:26.263597Z","shell.execute_reply.started":"2023-09-23T17:54:26.253306Z","shell.execute_reply":"2023-09-23T17:54:26.262545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_helper = ModelHelper(model=resnet_model, LR=0.0001)\n# resnet_model = model_helper.compile_model(optimizer_name='adam')\n# resnet_model.build(input_shape=INPUT_SHAPE_BUILD)\n# resnet_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.265100Z","iopub.execute_input":"2023-09-23T17:54:26.266229Z","iopub.status.idle":"2023-09-23T17:54:26.276417Z","shell.execute_reply.started":"2023-09-23T17:54:26.266197Z","shell.execute_reply":"2023-09-23T17:54:26.275508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# callbacks_lst = [model_helper.decrease_lr_on_plateau_callback()]\n# resnet_model_history = resnet_model.fit(train_dataset,\n#                                         steps_per_epoch=steps_per_epoch,\n#                                         epochs=100,\n#                                         verbose=1,\n#                                         callbacks=callbacks_lst,\n#                                         validation_data=val_datset,\n#                                         validation_steps=validation_steps,\n#                                         class_weight=None,\n#                                         workers=1,\n#                                         initial_epoch=0)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.278049Z","iopub.execute_input":"2023-09-23T17:54:26.278431Z","iopub.status.idle":"2023-09-23T17:54:26.289725Z","shell.execute_reply.started":"2023-09-23T17:54:26.278399Z","shell.execute_reply":"2023-09-23T17:54:26.288801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_helper.plot_model_train_info(resnet_model_history)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.303903Z","iopub.execute_input":"2023-09-23T17:54:26.304645Z","iopub.status.idle":"2023-09-23T17:54:26.308685Z","shell.execute_reply.started":"2023-09-23T17:54:26.304613Z","shell.execute_reply":"2023-09-23T17:54:26.307722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# resnet_info = model_helper.get_model_info(resnet_model_history, 'resnet')\n# resnet_info\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.309931Z","iopub.execute_input":"2023-09-23T17:54:26.310543Z","iopub.status.idle":"2023-09-23T17:54:26.319079Z","shell.execute_reply.started":"2023-09-23T17:54:26.310509Z","shell.execute_reply":"2023-09-23T17:54:26.318045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# resnet_model.evaluate(test_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.320596Z","iopub.execute_input":"2023-09-23T17:54:26.321314Z","iopub.status.idle":"2023-09-23T17:54:26.330800Z","shell.execute_reply.started":"2023-09-23T17:54:26.321281Z","shell.execute_reply":"2023-09-23T17:54:26.329750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# resnet_metrics = get_metrics_for_model(model=resnet_model, model_name='resnet')\n# resnet_metrics\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.332353Z","iopub.execute_input":"2023-09-23T17:54:26.333033Z","iopub.status.idle":"2023-09-23T17:54:26.342236Z","shell.execute_reply.started":"2023-09-23T17:54:26.333000Z","shell.execute_reply":"2023-09-23T17:54:26.341214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #\n# dump_model(resnet_model, 'G:/Learning/Degree Courses/MS AI ML/Research/Tea Sickeness Project/Code/output',\n#            'resnet_model_with_aug')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.343828Z","iopub.execute_input":"2023-09-23T17:54:26.344561Z","iopub.status.idle":"2023-09-23T17:54:26.353065Z","shell.execute_reply.started":"2023-09-23T17:54:26.344530Z","shell.execute_reply":"2023-09-23T17:54:26.351926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3 Mobilenet","metadata":{}},{"cell_type":"code","source":"# mobilenet = MobileNetV2(\n#     include_top=False, weights='imagenet', input_shape=INPUT_SHAPE)\n# mobilenet_model = ModelBuilder(pretrained_model_ref=mobilenet, num_classes=8)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.354661Z","iopub.execute_input":"2023-09-23T17:54:26.355379Z","iopub.status.idle":"2023-09-23T17:54:26.364233Z","shell.execute_reply.started":"2023-09-23T17:54:26.355346Z","shell.execute_reply":"2023-09-23T17:54:26.363141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_helper = ModelHelper(model=mobilenet_model, LR=0.0001)\n# mobilenet_model = model_helper.compile_model(optimizer_name='adam')\n# mobilenet_model.build(input_shape=INPUT_SHAPE_BUILD)\n# mobilenet_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.365789Z","iopub.execute_input":"2023-09-23T17:54:26.366498Z","iopub.status.idle":"2023-09-23T17:54:26.375654Z","shell.execute_reply.started":"2023-09-23T17:54:26.366466Z","shell.execute_reply":"2023-09-23T17:54:26.374641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# callbacks_lst = [model_helper.decrease_lr_on_plateau_callback()]\n\n# mobilenet_model_history = mobilenet_model.fit(train_dataset,\n#                                               steps_per_epoch=steps_per_epoch,\n#                                               epochs=100,\n#                                               verbose=1,\n#                                               callbacks=callbacks_lst,\n#                                               validation_data=val_datset,\n#                                               validation_steps=validation_steps,\n#                                               class_weight=None,\n#                                               workers=1,\n#                                               initial_epoch=0)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.377112Z","iopub.execute_input":"2023-09-23T17:54:26.378014Z","iopub.status.idle":"2023-09-23T17:54:26.386888Z","shell.execute_reply.started":"2023-09-23T17:54:26.377976Z","shell.execute_reply":"2023-09-23T17:54:26.385941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_helper.plot_model_train_info(mobilenet_model_history)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.388515Z","iopub.execute_input":"2023-09-23T17:54:26.389159Z","iopub.status.idle":"2023-09-23T17:54:26.402034Z","shell.execute_reply.started":"2023-09-23T17:54:26.389128Z","shell.execute_reply":"2023-09-23T17:54:26.401088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mobilenet_info = model_helper.get_model_info(\n#     mobilenet_model_history, 'mobilenet')\n# mobilenet_info\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.405553Z","iopub.execute_input":"2023-09-23T17:54:26.405883Z","iopub.status.idle":"2023-09-23T17:54:26.412498Z","shell.execute_reply.started":"2023-09-23T17:54:26.405859Z","shell.execute_reply":"2023-09-23T17:54:26.411621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mobilenet_model.evaluate(test_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.413867Z","iopub.execute_input":"2023-09-23T17:54:26.414307Z","iopub.status.idle":"2023-09-23T17:54:26.423840Z","shell.execute_reply.started":"2023-09-23T17:54:26.414238Z","shell.execute_reply":"2023-09-23T17:54:26.422823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mobilenet_metrics = get_metrics_for_model(\n#     model=mobilenet_model, model_name='mobilenet')\n# mobilenet_metrics\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.425205Z","iopub.execute_input":"2023-09-23T17:54:26.425649Z","iopub.status.idle":"2023-09-23T17:54:26.434141Z","shell.execute_reply.started":"2023-09-23T17:54:26.425617Z","shell.execute_reply":"2023-09-23T17:54:26.433142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #\n# dump_model(mobilenet_model, 'G:/Learning/Degree Courses/MS AI ML/Research/Tea Sickeness Project/Code/output',\n#            'mobilenet_model_with_aug')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.435434Z","iopub.execute_input":"2023-09-23T17:54:26.437858Z","iopub.status.idle":"2023-09-23T17:54:26.446913Z","shell.execute_reply.started":"2023-09-23T17:54:26.437832Z","shell.execute_reply":"2023-09-23T17:54:26.445834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.6 Comparing models trained on real + geometrically augmented data","metadata":{}},{"cell_type":"code","source":"\n# all_model_info = pd.concat(\n#     [densenet_info, resnet_info, mobilenet_info], axis=1)\n# all_model_metrics = pd.concat(\n#     [densenet_metrics, resnet_metrics, mobilenet_metrics], axis=1)\n# evaulation_info = pd.concat([all_model_info, all_model_metrics], axis=0)\n# evaulation_info\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.450460Z","iopub.execute_input":"2023-09-23T17:54:26.450784Z","iopub.status.idle":"2023-09-23T17:54:26.459999Z","shell.execute_reply.started":"2023-09-23T17:54:26.450759Z","shell.execute_reply":"2023-09-23T17:54:26.458979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaulation_info.plot.bar(figsize=[7, 5], grid=True, fontsize=14)\n# plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.461633Z","iopub.execute_input":"2023-09-23T17:54:26.462014Z","iopub.status.idle":"2023-09-23T17:54:26.471615Z","shell.execute_reply.started":"2023-09-23T17:54:26.461982Z","shell.execute_reply":"2023-09-23T17:54:26.470706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6 GANs","metadata":{}},{"cell_type":"markdown","source":"#### 6.1.1 Constants and hyperparameters for GAN","metadata":{}},{"cell_type":"code","source":"# global\nnum_channels = 3\nnum_classes = 8\n\n# for conditional gan\nlatent_dim = 128\n\n# for wgan & wgan-sn\nnoise_dim = 256\n\n# Set the number of epochs for trainining.\nwgan_epochs = 200\n\ntrain_wgan = True\ntrain_wgan_res = True\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:26.473222Z","iopub.execute_input":"2023-09-23T17:54:26.473596Z","iopub.status.idle":"2023-09-23T17:54:26.483462Z","shell.execute_reply.started":"2023-09-23T17:54:26.473564Z","shell.execute_reply":"2023-09-23T17:54:26.482237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator_in_channels = latent_dim + num_classes\ndiscriminator_in_channels = num_channels + num_classes\nprint(generator_in_channels, discriminator_in_channels)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.484909Z","iopub.execute_input":"2023-09-23T17:54:26.485526Z","iopub.status.idle":"2023-09-23T17:54:26.496705Z","shell.execute_reply.started":"2023-09-23T17:54:26.485495Z","shell.execute_reply":"2023-09-23T17:54:26.495804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#### 6.1.2 Save the model weights and outputs into zip file","metadata":{}},{"cell_type":"code","source":"\ndef get_all_file_as_zip(model_history, gan_name='wgan', delete_prev=False):\n\n    if delete_prev:\n        if os.path.exists('./output'):\n            shutil.rmtree('./output')\n\n        if os.path.exists('./output.zip'):\n            os.remove('./output.zip')\n\n        if not os.path.exists('./output'):\n            os.makedirs('./output')\n\n    timestamp = (datetime.datetime.now() + datetime.timedelta(hours=5,\n                 minutes=30)).strftime('%d_%m_%y_%H-%M-%S')\n    pd.DataFrame(model_history.history).to_csv(\n        f'./output/{gan_name}_history_{timestamp}.csv')\n\n    shutil.move('./checkpoints', './output', )\n\n    shutil.make_archive(f'output_{gan_name}_{timestamp}', 'zip', './output')\n\n    os.chdir(r'/kaggle/working')\n    FileLinks('./')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:26.498364Z","iopub.execute_input":"2023-09-23T17:54:26.499140Z","iopub.status.idle":"2023-09-23T17:54:26.507846Z","shell.execute_reply.started":"2023-09-23T17:54:26.499106Z","shell.execute_reply":"2023-09-23T17:54:26.506743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2 Streamlining the data loading for gan training","metadata":{}},{"cell_type":"code","source":"gan_train_img_path_lst, _ = get_train_test_fp_list(sickness_folder_processed_dict,\n                                                   train_split_ratio=1.0,\n                                                   test_split_ratio=0.0)\n\nprint(f'Train images # -> {len(gan_train_img_path_lst)}')\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:26.509333Z","iopub.execute_input":"2023-09-23T17:54:26.509953Z","iopub.status.idle":"2023-09-23T17:54:26.531908Z","shell.execute_reply.started":"2023-09-23T17:54:26.509922Z","shell.execute_reply":"2023-09-23T17:54:26.530527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.3 Load the GAN training dataset","metadata":{}},{"cell_type":"code","source":"gan_train_data_df = get_img_path_and_labels_df(gan_train_img_path_lst)\n\n\ngan_train_data_df.head()\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:26.533363Z","iopub.execute_input":"2023-09-23T17:54:26.533891Z","iopub.status.idle":"2023-09-23T17:54:26.596913Z","shell.execute_reply.started":"2023-09-23T17:54:26.533858Z","shell.execute_reply":"2023-09-23T17:54:26.595730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gan_paths_X_train = gan_train_data_df['paths'].values.tolist()\n\n[gan_paths_X_train[i] for i in np.random.randint(\n    low=0, high=len(gan_paths_X_train)-1, size=5)]\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:26.598513Z","iopub.execute_input":"2023-09-23T17:54:26.599003Z","iopub.status.idle":"2023-09-23T17:54:26.607524Z","shell.execute_reply.started":"2023-09-23T17:54:26.598949Z","shell.execute_reply":"2023-09-23T17:54:26.606480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# .astype(np.float32)\ngan_lables_y_train = gan_train_data_df[gan_train_data_df['disease'].unique(\n)].values\n\n[gan_lables_y_train[i] for i in np.random.randint(\n    low=0, high=len(gan_lables_y_train)-1, size=5)]\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:26.609245Z","iopub.execute_input":"2023-09-23T17:54:26.609996Z","iopub.status.idle":"2023-09-23T17:54:26.623342Z","shell.execute_reply.started":"2023-09-23T17:54:26.609953Z","shell.execute_reply":"2023-09-23T17:54:26.622097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gan_train_dataset = gen_dataset(\n    gan_paths_X_train, gan_lables_y_train, batch_size=BATCH_SIZE_DATASET)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:26.625275Z","iopub.execute_input":"2023-09-23T17:54:26.626061Z","iopub.status.idle":"2023-09-23T17:54:29.620082Z","shell.execute_reply.started":"2023-09-23T17:54:26.626007Z","shell.execute_reply":"2023-09-23T17:54:29.619105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_gan_X_train_data, sample_gan_y_train_data = next(\n    iter(gan_train_dataset))\nsample_gan_X_train_data.shape, sample_gan_y_train_data.shape\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:29.621626Z","iopub.execute_input":"2023-09-23T17:54:29.621965Z","iopub.status.idle":"2023-09-23T17:54:32.780426Z","shell.execute_reply.started":"2023-09-23T17:54:29.621932Z","shell.execute_reply":"2023-09-23T17:54:32.779477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.5 Wasserstein GAN (WGAN) with Gradient Penalty (GP)\n\n\nInfo: https://arxiv.org/abs/1701.07875","metadata":{}},{"cell_type":"markdown","source":"#### 6.5.1 WGAN-Discriminator","metadata":{}},{"cell_type":"code","source":"def build_discriminator_wgan(discriminator_in_channels):\n    discriminator = Sequential(\n        [\n            InputLayer(input_shape=(180, 180, discriminator_in_channels)),\n            layers.ZeroPadding2D((2, 2)),\n            Conv2D(filters=64, kernel_size=(5, 5), strides=(2, 2),\n                   use_bias=True, activation=layers.LeakyReLU(0.2), padding='same'),\n            Conv2D(filters=128, kernel_size=(5, 5), strides=(2, 2),\n                   use_bias=True, activation=layers.LeakyReLU(0.2), padding='same'),\n            layers.Dropout(0.3),\n            Conv2D(filters=256, kernel_size=(5, 5), strides=(2, 2),\n                   use_bias=True, activation=layers.LeakyReLU(0.2), padding='same'),\n            layers.Dropout(0.3),\n            Conv2D(filters=512, kernel_size=(5, 5), strides=(2, 2),\n                   use_bias=True, activation=layers.LeakyReLU(0.2)),\n            Flatten(),\n            Dropout(0.2),\n            Dense(256, activation='relu', name = \"full_con_1\"),\n            Dropout(0.1),\n            Dense(128, activation='relu', name = \"full_con_2\"),\n            Dropout(0.1),\n            Dense(1, activation=LeakyReLU(), name = \"output\"),\n        ],\n        name=\"discrinator_wgan\"\n    )\n\n    return discriminator","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:32.782115Z","iopub.execute_input":"2023-09-23T17:54:32.782704Z","iopub.status.idle":"2023-09-23T17:54:32.795987Z","shell.execute_reply.started":"2023-09-23T17:54:32.782668Z","shell.execute_reply":"2023-09-23T17:54:32.794934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nd_model = build_discriminator_wgan(discriminator_in_channels)\nd_model.summary()\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:32.797388Z","iopub.execute_input":"2023-09-23T17:54:32.798415Z","iopub.status.idle":"2023-09-23T17:54:33.319138Z","shell.execute_reply.started":"2023-09-23T17:54:32.798381Z","shell.execute_reply":"2023-09-23T17:54:33.318327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(d_model, show_shapes=True,\n                          to_file='d_model.png', dpi=70)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:33.320223Z","iopub.execute_input":"2023-09-23T17:54:33.320649Z","iopub.status.idle":"2023-09-23T17:54:33.575058Z","shell.execute_reply.started":"2023-09-23T17:54:33.320622Z","shell.execute_reply":"2023-09-23T17:54:33.572043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.5.2 WGAN - Attention Network","metadata":{}},{"cell_type":"code","source":"def generate_mobilenet_feature_extrator(slice_layer_number=53, show_summary=False):\n    mv = MobileNetV2(include_top=False, weights='imagenet',\n                     input_shape=(180, 180, 3))\n    for layers in mv.layers:\n        layers.trainable = False\n    _input = mv.input\n    _output = mv.layers[slice_layer_number].output\n\n    attn_model_feature_extractor = keras.Model(\n        inputs=_input, outputs=_output, name='attn_model_feature_extractor')\n    if show_summary:\n        attn_model_feature_extractor.summary()\n\n    return attn_model_feature_extractor\n\n\ndef conv_layers(kernels):\n    return Conv2D(filters=kernels, kernel_size=(3, 3))\n\n\nclass AttentionModel(Model):\n    \"\"\" Model to generate the context vector by using feature map and hidden state of decoder \"\"\"\n\n    def __init__(self, units):\n        self.units = units\n        super(AttentionModel, self).__init__()\n        # Dense layer to take inputs of pretrained model\n\n        self.pretrained_model = generate_mobilenet_feature_extrator(\n            slice_layer_number=53)\n\n        self.conv_layers_1 = conv_layers(self.units)\n        self.bn_1 = keras.layers.BatchNormalization()\n\n        self.conv_layers_2 = conv_layers(self.units)\n        self.bn_2 = keras.layers.BatchNormalization()\n\n        self.W1 = tf.keras.layers.Dense(self.units, name='1st_dense')\n\n        # Dense layer having one neuron to hold the score of a context vector\n        self.V = tf.keras.layers.Dense(1, name='score')\n\n        # self.aggregate_dense = tf.keras.layers.Dense(units)\n\n    def call(self, imgs):\n\n        feature_maps = self.pretrained_model(imgs)\n        features = self.conv_layers_1(feature_maps)\n        features = self.bn_1(features)\n        features = PixelAttention2D(\n            features.shape[-1], name=\"pix-atten-1\")(features)\n\n        features = self.conv_layers_2(features)\n        features = self.bn_2(features)\n        features = PixelAttention2D(\n            features.shape[-1], name=\"pix-atten-2\")(features)\n\n        # build your score funciton to shape: (batch_size, 8*8, units)\n        attention_hidden_layer = tf.keras.activations.tanh(self.W1(features))\n\n        # score shape == (batch_size, 64, 1)\n        # This gives you an unnormalized score for each image feature.\n        score = self.V(attention_hidden_layer)\n\n        # extract your attention weights with shape: (batch_size, 8*8, 1)\n        attention_weights = tf.keras.activations.softmax(score, axis=1)\n\n        # shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n        # context_vector = attention_weights * features\n        context_vector = tf.matmul(\n            attention_weights, features, transpose_a=True)\n\n        context_vector = tf.reshape(\n            context_vector, [-1, 19, 32])\n\n        return context_vector\n\n    def summary(self):\n        x = keras.layers.Input(shape=(180, 180, 3))\n        model = keras.Model(inputs=[x], outputs=self.call(x))\n        return model.summary()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:33.576518Z","iopub.execute_input":"2023-09-23T17:54:33.576875Z","iopub.status.idle":"2023-09-23T17:54:33.593411Z","shell.execute_reply.started":"2023-09-23T17:54:33.576841Z","shell.execute_reply":"2023-09-23T17:54:33.592287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"am = AttentionModel(32)\nam.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:33.595238Z","iopub.execute_input":"2023-09-23T17:54:33.595890Z","iopub.status.idle":"2023-09-23T17:54:35.662545Z","shell.execute_reply.started":"2023-09-23T17:54:33.595857Z","shell.execute_reply":"2023-09-23T17:54:35.661781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.5.3 WGAN - Generator","metadata":{}},{"cell_type":"code","source":"def upsample_block(x,\n                   filters,\n                   activation,\n                   kernel_size=(3, 3),\n                   strides=(1, 1),\n                   up_size=(2, 2),\n                   padding=\"same\",\n                   use_bn=False,\n                   use_sn=False,\n                   use_bias=True,\n                   use_dropout=False,\n                   use_gn = False,\n                   drop_value=0.3,\n                   name=''\n                   ):\n\n    x = layers.UpSampling2D(up_size, name=name + '-upsample')(x)\n\n    # if spectral normalization is enabled\n    if use_sn:\n        conv2d_sn = tfa.layers.SpectralNormalization(layers.Conv2D(\n            filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias, name=name+'-con2d'))\n        x = conv2d_sn(x)\n    else:\n        x = layers.Conv2D(filters, kernel_size, strides=strides,\n                          padding=padding, use_bias=use_bias, name=name+'-con2d')(x)\n\n    if use_bn:\n        x = layers.BatchNormalization()(x)\n    if use_gn:\n        x = layers.GroupNormalization()(x)\n\n    if activation:\n        x = activation(x)\n    if use_dropout:\n        x = layers.Dropout(drop_value)(x)\n    return x\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:35.663654Z","iopub.execute_input":"2023-09-23T17:54:35.664335Z","iopub.status.idle":"2023-09-23T17:54:35.682635Z","shell.execute_reply.started":"2023-09-23T17:54:35.664279Z","shell.execute_reply":"2023-09-23T17:54:35.681838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_generator_model(add_residiual=False):\n    noise = layers.Input(shape=(noise_dim,), name='noise')\n    labels = layers.Input(shape=(num_classes,), name='lables')\n    images = layers.Input(shape=(180, 180, 3), name='images')\n\n    attention_vectors = AttentionModel(units=32)(images)\n\n    if add_residiual:\n        # Residual attention feed-fwd\n        residual_atten_vectors = attention_vectors\n        residual_atten_vectors = tf.keras.layers.Conv1D(\n            filters=60, kernel_size=3, activation='relu', name='residual_conv1')(residual_atten_vectors)\n        residual_atten_vectors = tf.keras.layers.PReLU(alpha_initializer= \"zeros\", name = \"Preule_1\")(residual_atten_vectors)\n        residual_atten_vectors = tf.keras.layers.UpSampling1D()(residual_atten_vectors)\n        residual_atten_vectors = tf.keras.layers.Conv1D(\n            filters=30, kernel_size=5, activation='relu', name='residual_conv2')(residual_atten_vectors)\n        residual_atten_vectors = tf.keras.layers.PReLU(alpha_initializer= \"zeros\", name  = \"Preule_2\")(residual_atten_vectors)\n        residual_atten_vectors = tf.keras.layers.BatchNormalization(\n            name='residual_bn')(residual_atten_vectors)\n\n    batch_size_train = tf.shape(images)[0]\n\n    # shape = TensorShape([4, 8, 32])\n    random_noise_vectors = tf.reshape(noise, shape=(batch_size_train, 8, 32))\n\n    # shape = TensorShape([4, 19, 8]) -> random_noise_vectors x attention_vectors\n    attention_vectors_with_noise = tf.matmul(\n        random_noise_vectors, attention_vectors, transpose_b=True, name='noise_matmul_atten_vectors')\n\n    # shape = TensorShape([4, 152])\n    attention_vectors_with_noise = tf.reshape(\n        attention_vectors_with_noise, shape=(batch_size_train, 152))\n\n    # shape = TensorShape([4, 160])\n    input_data = tf.concat([attention_vectors_with_noise, labels], axis=1)\n\n    x = layers.Dense(15 * 15 * 256, use_bias=False)(input_data)  # 57600\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n\n    if add_residiual:\n        # concatenating the residual_atten_vectors\n        residual_atten_vectors = Flatten(name='residual_flatten')(residual_atten_vectors)  # 900\n        x = tf.concat([x, residual_atten_vectors],\n                      axis=1, name='concat_residual')\n\n        x = layers.Reshape((15, 15, 260), name='residual_reshape')(x)  # 58050\n\n    else:\n        x = layers.Reshape((15, 15, 256))(x)  # 58050\n\n    x = upsample_block(x, 128, layers.LeakyReLU(0.2), strides=(1, 1), up_size=(\n        4, 4), use_bias=False, use_gn=True, padding=\"same\", use_dropout=False, name='1')\n\n    x = upsample_block(x, 64, layers.LeakyReLU(0.2), strides=(1, 1), use_bias=False,\n                       use_gn=True, up_size=(3, 3), padding=\"same\", use_dropout=False, name='2')\n\n    x = upsample_block(x, 3, layers.Activation(\"tanh\"), up_size=(\n        1, 1), strides=(1, 1), use_bias=False, use_bn=True, name='3')\n\n    g_model = keras.models.Model([noise, labels, images], x, name=\"generator\")\n    return g_model\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-23T17:54:35.683855Z","iopub.execute_input":"2023-09-23T17:54:35.684501Z","iopub.status.idle":"2023-09-23T17:54:35.708476Z","shell.execute_reply.started":"2023-09-23T17:54:35.684430Z","shell.execute_reply":"2023-09-23T17:54:35.707603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ng_model = get_generator_model(add_residiual=False)\ng_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:35.709838Z","iopub.execute_input":"2023-09-23T17:54:35.710475Z","iopub.status.idle":"2023-09-23T17:54:37.631201Z","shell.execute_reply.started":"2023-09-23T17:54:35.710444Z","shell.execute_reply":"2023-09-23T17:54:37.630503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(g_model, show_shapes=True,\n                          show_layer_names=True,  to_file='g_model.png', dpi=70)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:37.632309Z","iopub.execute_input":"2023-09-23T17:54:37.632871Z","iopub.status.idle":"2023-09-23T17:54:37.855270Z","shell.execute_reply.started":"2023-09-23T17:54:37.632835Z","shell.execute_reply":"2023-09-23T17:54:37.854246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.5.4 Build WGAN model ","metadata":{}},{"cell_type":"code","source":"class WGAN(keras.Model):\n    def __init__(self,discriminator,generator,latent_dim,discriminator_extra_steps=5,gp_weight=10.0,):\n\n        super(WGAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n        self.d_steps = discriminator_extra_steps\n        self.gp_weight = gp_weight\n        self.attention_block = AttentionModel(units=32)\n        self.preprocess_mobilenet = tf.keras.applications.mobilenet_v2.preprocess_input\n\n    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n        super(WGAN, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.d_loss_fn = d_loss_fn\n        self.g_loss_fn = g_loss_fn\n\n    def gradient_penalty(self, batch_size, real_images, fake_images):\n        \"\"\"Calculates the gradient penalty.\n\n        This loss is calculated on an interpolated image\n        and added to the discriminator loss.\n        \"\"\"\n        # Get the interpolated image\n        alpha = tf.random.normal(shape=[batch_size, 1, 1, 1], mean=0.0, stddev=1.0)\n        diff = fake_images - real_images\n        interpolated = real_images + alpha * diff\n\n        with tf.GradientTape() as gp_tape:\n            gp_tape.watch(interpolated)\n            # 1. Get the discriminator output for this interpolated image.\n            pred = self.discriminator(interpolated, training=True)\n\n        # 2. Calculate the gradients w.r.t to this interpolated image.\n        grads = gp_tape.gradient(pred, [interpolated])[0]\n        # 3. Calculate the norm of the gradients.\n        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n        gp = tf.reduce_mean((norm - 1.0) ** 2)\n        return gp\n\n    def train_step(self, data, image_size=IMG_SIZE_TRAIN[0], num_classes=8):\n\n        # Unpack the data.\n        real_images, one_hot_labels = data\n        batch_size_train = tf.shape(real_images)[0]\n\n        real_images = self.preprocess_mobilenet(real_images)\n\n        # Add dummy dimensions to the labels so that they can be concatenated with\n        # the images. This is for the discriminator.\n        image_one_hot_labels = one_hot_labels[:, :, None, None]\n        image_one_hot_labels = tf.repeat(one_hot_labels, repeats=[image_size * image_size])\n\n        image_one_hot_labels = tf.reshape(image_one_hot_labels, (-1, image_size, image_size, num_classes))\n\n        ##################################\n        ##### Discriminator Training #####\n        ##################################\n\n        for _ in range(self.d_steps):\n            # shape = TensorShape([4, 256])\n            random_latent_vectors = tf.random.normal(shape=(batch_size_train, self.latent_dim))\n\n            # Train the discriminator.\n            with tf.GradientTape() as tape:\n\n                # Decode the noise (guided by labels) to fake images.\n                # shape = (4, 180, 180, 3)\n                generated_images_fake = self.generator([random_latent_vectors, one_hot_labels, real_images], training=True)\n\n                # Combine them with real images. Note that we are concatenating the labels\n                # with these images here.\n                # shape = TensorShape([4, 180, 180, 11])\n                fake_image_and_labels = tf.concat([generated_images_fake, image_one_hot_labels], -1)\n\n                real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n\n                predictions_on_fake = self.discriminator(fake_image_and_labels, training=True)\n                predictions_on_real = self.discriminator(real_image_and_labels, training=True)\n\n                d_loss_disc = self.d_loss_fn(real_data=predictions_on_real, pred_data=predictions_on_fake)\n                #gp = self.gradient_penalty(batch_size, real_images, generated_images)\n                gp = self.gradient_penalty(batch_size_train, real_image_and_labels, fake_image_and_labels)\n\n                # Add the gradient penalty to the original discriminator loss\n                d_loss = d_loss_disc + gp * self.gp_weight\n\n            # Get gradients wrt disc loss\n            d_grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n\n            # Update discriminator weights\n            self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_weights))\n\n        ##############################\n        ##### Generator Training #####\n        ##############################\n\n        # Sample random points in the latent space.\n        # shape = TensorShape([4, 256])\n        random_latent_vectors = tf.random.normal(shape=(batch_size_train, self.latent_dim))\n\n        # Train the generator (note that we should *not* update the weights\n        # of the discriminator)!\n        with tf.GradientTape() as tape:\n\n            generated_images = self.generator([random_latent_vectors, one_hot_labels, real_images], training=True)\n\n            fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n\n            predictions = self.discriminator(fake_image_and_labels, training=True)\n\n            g_loss = self.g_loss_fn(predictions)\n\n        # Update the generator weights\n        gen_grads = tape.gradient(g_loss, self.generator.trainable_weights)\n        self.g_optimizer.apply_gradients(zip(gen_grads, self.generator.trainable_weights))\n\n        return {\"g_loss\": g_loss,\"d_loss\": d_loss}\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:37.857163Z","iopub.execute_input":"2023-09-23T17:54:37.857533Z","iopub.status.idle":"2023-09-23T17:54:37.880612Z","shell.execute_reply.started":"2023-09-23T17:54:37.857499Z","shell.execute_reply":"2023-09-23T17:54:37.879431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.5.5 WGAN - Model save checkpoint","metadata":{}},{"cell_type":"code","source":"class SaveCheckpoint(keras.callbacks.Callback):\n    def __init__(self, filepath, epoch_to_wait=5):\n        self._filepath = filepath\n        self._epoch_to_wait = epoch_to_wait\n\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch % self._epoch_to_wait == 0:\n            timestamp = (datetime.datetime.now(\n            ) + datetime.timedelta(hours=5, minutes=30)).strftime('%d_%m_%y_%H-%M-%S')\n            folder_path = os.path.join(\n                self._filepath, f'epoch_{epoch}_{timestamp}')\n            os.makedirs(folder_path)\n            # saving in h5 format\n            self.model.save_weights(f'{folder_path}/wgan_ckp_epoch_{epoch}.h5')\n            print(f'\\tCheckpoint saved for epoch {epoch} at -> {folder_path}')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:37.883329Z","iopub.execute_input":"2023-09-23T17:54:37.883730Z","iopub.status.idle":"2023-09-23T17:54:37.899216Z","shell.execute_reply.started":"2023-09-23T17:54:37.883698Z","shell.execute_reply":"2023-09-23T17:54:37.898214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.5.6 WGAN - Image save callback","metadata":{}},{"cell_type":"code","source":"class ImageSaveCallback(keras.callbacks.Callback):\n    def __init__(self, disease_lst, num_img, latent_dim, img_dump_dir, epoch_to_wait ):\n        self.disease_lst = disease_lst\n        self.num_img = num_img\n        self.latent_dim = latent_dim\n        self.img_dump_dir = img_dump_dir\n        self.gan_train_dataset = gen_dataset(\n            gan_paths_X_train, gan_lables_y_train, batch_size=num_img)\n        self.epoch_to_wait = epoch_to_wait\n\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch % self.epoch_to_wait == 0):\n            timestamp = (datetime.datetime.now(\n            ) + datetime.timedelta(hours=5, minutes=30)).strftime('%d_%m_%y_%H-%M-%S')\n            sample_gan_X_train_data, sample_gan_y_train_data = next(\n                iter(self.gan_train_dataset))\n\n            sample_gan_X_train_data = tf.keras.applications.mobilenet_v2.preprocess_input(\n                sample_gan_X_train_data)\n\n            batch_size_train = tf.shape(sample_gan_X_train_data)[0]\n\n            random_latent_vectors = tf.random.normal(\n                shape=(batch_size_train, self.latent_dim))\n\n            generated_images = self.model.generator(\n                [random_latent_vectors, sample_gan_y_train_data, sample_gan_X_train_data])\n            generated_images = (generated_images * 127.5) + 127.5\n\n            for i, label in enumerate(sample_gan_y_train_data):\n                idx = np.argmax(label)\n                disease = self.disease_lst[idx]\n                img = generated_images[i].numpy()\n                img = keras.preprocessing.image.array_to_img(img)\n\n                disease_path = f'{self.img_dump_dir}/{disease}'\n\n                if not os.path.exists(disease_path):\n                    os.makedirs(disease_path)\n\n                img.save(\n                    f\"{disease_path}/{disease}_{i}_{epoch}_{timestamp}.jpg\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T17:54:37.900688Z","iopub.execute_input":"2023-09-23T17:54:37.901153Z","iopub.status.idle":"2023-09-23T17:54:37.913718Z","shell.execute_reply.started":"2023-09-23T17:54:37.901122Z","shell.execute_reply":"2023-09-23T17:54:37.912660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.5.7 Training WGAN model","metadata":{}},{"cell_type":"markdown","source":"##### 6.5.7.1 Training - Without residual connection","metadata":{}},{"cell_type":"code","source":"# (learning_rate=0.0002, beta_1=0.5 are recommended)\n\n# generator optimizers - lr = 0.0001, beta_1 = 0.5. beta_2 = 0.9\ngenerator_optimizer = keras.optimizers.Adam(\n    learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n\n# discriminator optimizers - lr = 0.0004, beta_1 = 0.5. beta_2 = 0.9\ndiscriminator_optimizer = keras.optimizers.Adam(\n    learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n\n\ndef discriminator_loss(real_data, pred_data):\n    real_loss = tf.reduce_mean(real_data)\n\n    fake_loss = tf.reduce_mean(pred_data)\n    return fake_loss - real_loss\n\n\n# Define the loss functions for the generator.\ndef generator_loss(fake_img):\n    return -tf.reduce_mean(fake_img)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T18:01:45.127288Z","iopub.execute_input":"2023-09-23T18:01:45.127657Z","iopub.status.idle":"2023-09-23T18:01:45.138324Z","shell.execute_reply.started":"2023-09-23T18:01:45.127629Z","shell.execute_reply":"2023-09-23T18:01:45.137369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Callback configuration","metadata":{}},{"cell_type":"code","source":"# Instantiate the custom `ImageSaveCallback` Keras callback.\ncbk = ImageSaveCallback(num_img=BATCH_SIZE_GAN_TRAIN,\n                        latent_dim=noise_dim,\n                        disease_lst=train_data_df['disease'].unique().tolist(),\n                        img_dump_dir='./output/wgan',\n                        epoch_to_wait=5)\n\n# fid = FID()\n# inception_score = IS()\ncheckpoint = SaveCheckpoint(\n    filepath=MODEL_CHECKPOINT_PATHS['wgan'],\n    epoch_to_wait=5)\n\n# Get the wgan model\nwgan = WGAN(discriminator=d_model,\n            generator=g_model,\n            latent_dim=noise_dim,\n            discriminator_extra_steps=5)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T18:01:46.172208Z","iopub.execute_input":"2023-09-23T18:01:46.172606Z","iopub.status.idle":"2023-09-23T18:01:47.451208Z","shell.execute_reply.started":"2023-09-23T18:01:46.172577Z","shell.execute_reply":"2023-09-23T18:01:47.450247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_wgan:\n\n    # # Compile the wgan model\n    wgan.compile(d_optimizer=discriminator_optimizer,\n                 g_optimizer=generator_optimizer,\n                 g_loss_fn=generator_loss,\n                 d_loss_fn=discriminator_loss)\n\n    # Start training\n    wgan_hist = wgan.fit(gan_train_dataset,\n                         batch_size=BATCH_SIZE_GAN_TRAIN,\n                         epochs=200,\n                         callbacks=[checkpoint, cbk])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T18:01:47.453238Z","iopub.execute_input":"2023-09-23T18:01:47.453618Z","iopub.status.idle":"2023-09-23T21:12:14.194151Z","shell.execute_reply.started":"2023-09-23T18:01:47.453583Z","shell.execute_reply":"2023-09-23T21:12:14.193159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_wgan:\n    pd.DataFrame(wgan_hist.history).plot(grid=True)4\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if kaggle:\n    get_all_file_as_zip(model_history=wgan_hist, gan_name='wgan')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 6.5.7.2 Training - With residual connection","metadata":{}},{"cell_type":"code","source":"g_model_with_residual = get_generator_model(add_residiual=True)\ng_model_with_residual.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(g_model_with_residual, show_shapes=True,\n                          show_layer_names=True,  to_file='g_model_with_residual.png', dpi=70)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate the custom `ImageSaveCallback` custom callback.\ncbk = ImageSaveCallback(num_img=BATCH_SIZE_GAN_TRAIN,\n                        latent_dim=noise_dim,\n                        disease_lst=train_data_df['disease'].unique().tolist(),\n                        img_dump_dir='./output/wgan_res')\n\n# Instantiate the custom `SaveCheckpoint` custom callback.\ncheckpoint = SaveCheckpoint(filepath=MODEL_CHECKPOINT_PATHS['wgan_res'],\n                            epoch_to_wait=5)\n\n# Get the wgan model\nwgan_res = WGAN(discriminator=d_model,\n                generator=g_model_with_residual,\n                latent_dim=noise_dim,\n                discriminator_extra_steps=5,\n                gp_weight=20)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_wgan_res:\n    # # Compile the wgan model\n    wgan_res.compile(d_optimizer=discriminator_optimizer,\n                     g_optimizer=generator_optimizer,\n                     g_loss_fn=generator_loss,\n                     d_loss_fn=discriminator_loss)\n\n    # # Start training\n    wgan_res_hist = wgan_res.fit(\n        gan_train_dataset, batch_size=BATCH_SIZE_GAN_TRAIN, epochs=150, callbacks=[checkpoint, cbk])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_wgan_res:\n    pd.DataFrame(wgan_res_hist.history).plot(grid=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if kaggle:\n    get_all_file_as_zip(model_history=wgan_res_hist, gan_name='wgan_res')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7 Generate images for each classes","metadata":{}},{"cell_type":"markdown","source":"*Generate 100 images for each class which will be used by the proportional ratio mixing mechanism to train a pretrained model following:*\n\n     real + synthetic images\n     real + geometrical augmented + synthetic images","metadata":{}},{"cell_type":"markdown","source":"### 7.1 Loading the data paths for sampling and shuffling","metadata":{}},{"cell_type":"code","source":"actual_data_df\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual_data_df.disease.value_counts()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef get_synthetic_img_generation_paths(df, disease, sample_size=100):\n    disease_df = df[df['disease'] == disease]\n    disease_df_sample = disease_df.sample(\n        n=sample_size, axis=0, replace=True, random_state=42)\n    disease_df = pd.concat([disease_df, disease_df_sample], axis=0)\n    return disease_df\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_data_df = pd.DataFrame()\nfor disease in actual_data_df['disease'].unique():\n    sample_data_df = pd.concat([sample_data_df, get_synthetic_img_generation_paths(\n        actual_data_df, disease)], axis=0)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_data_df.disease.value_counts()  # .plot.bar(color = 'y')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffle the data\n\nsample_data_df = sample_data_df.sample(frac=1)\nsample_data_df\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths_X_train_syn_gen = sample_data_df['paths'].values.tolist()\nlables_y_train_syn_gen = sample_data_df[sample_data_df['disease'].unique(\n)].values\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths_X_train_syn_gen[:5]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.2 Generate synthetic images","metadata":{}},{"cell_type":"code","source":"class SyntheticImageGenerator():\n    def __init__(self, model_ref, checkpoint_path, data_dump_dir):\n        self._model_ref = model_ref\n        self._checkpoint_path = checkpoint_path\n        self._data_dump_dir = data_dump_dir\n        self._image_path_dict = {}\n\n        if self._model_ref:\n            self.load_model_weights()\n            print(\n                f'INFO: Model loaded from from checkpoint -> {self._checkpoint_path}')\n\n    def load_model_weights(self):\n        try:\n            self._model_ref.built = True\n            self._model_ref.load_weights(self._checkpoint_path)\n        except:\n            raise Exception(\n                f'Model load error!!!\\nCheckpoint path: {self._checkpoint_path}')\n\n    def generate_images(self, _noise, _real_data, _labels,):\n\n        _fake_data = self._model_ref.generator([_noise, _labels, _real_data])\n\n        return _fake_data\n\n    def map_func_for_generation(self, image_path, lables):\n        \"\"\" This function will take the image_path & caption and return it's feature & respective caption. \"\"\"\n\n        img_tensor = skimage.io.imread(image_path)  # .decode('utf-8'))\n        img_tensor = img_tensor.astype(np.float32)\n        lables = lables.astype(np.float32)\n\n        return img_tensor, lables\n\n    def handle_image_generation_and_dump(self, img_path, label, disease_lst):\n        img_name = os.path.basename(img_path)\n        X, y = self.map_func_for_generation(image_path=img_path, lables=label)\n\n        X = np.expand_dims(X, axis=0)\n        y = y.reshape(1, -1)\n\n        noise = tf.random.normal(shape=(1, 256))\n        fake_data = self.generate_images(_noise=noise, _real_data=X, _labels=y)\n\n        for i, data in enumerate(fake_data):\n            idx = np.argmax(y[i])\n            disease = disease_lst[idx]\n            img_data = data.numpy()\n\n            img_data = (img_data*127.5) + 127.5\n\n            img_data = tf.keras.preprocessing.image.array_to_img(img_data)\n\n            disease_path = f'{self._data_dump_dir}/{disease}/generated'\n\n            if not os.path.exists(disease_path):\n                os.makedirs(disease_path)\n\n            img_path = f\"{disease_path}/{img_name}\"\n\n            # logic for handling duplicate image samples\n            if os.path.exists(img_path):\n                _path, _format = os.path.splitext(img_path)\n\n                _path = _path + '_'\n                img_path = f'{_path}{_format}'\n\n                if os.path.exists(img_path):\n                    while os.path.exists(img_path):\n                        _path = _path + '_'\n                        img_path = f'{_path}{_format}'\n\n            # if disease in self._image_path_dict:\n            #     self._image_path_dict[disease].append(img_path)\n            # else:\n            #     self._image_path_dict[disease] = img_path\n\n            img_data.save(img_path)\n\n    def handle_synthetic_image_generation(self, disease_lst, paths_X_train_syn_gen, lables_y_train_syn_gen):\n\n        t1 = time.perf_counter()\n\n        for img_path, label in tqdm(zip(paths_X_train_syn_gen, lables_y_train_syn_gen), desc='Generating synthetic images...'):\n\n            self.handle_image_generation_and_dump(\n                img_path=img_path, label=label, disease_lst=disease_lst)\n\n        print(f'\\nDumped images are in -> {self._data_dump_dir}')\n        print(f'Elapsed time : {time.perf_counter() - t1}')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7.2.1 Using wgan","metadata":{}},{"cell_type":"code","source":"syn_img_gen = SyntheticImageGenerator(wgan,\n                                      checkpoint_path='./checkpoints/vanilaa as h5 fromat/checkpoints/wgan/epoch_98_12_11_22_21-57-02/wgan_ckp_epoch_98.h5',\n                                      data_dump_dir='./tmp/dump_vanilla')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"syn_img_gen.handle_synthetic_image_generation(disease_lst=sample_data_df['disease'].unique().tolist(),\n                                              paths_X_train_syn_gen=paths_X_train_syn_gen,\n                                              lables_y_train_syn_gen=lables_y_train_syn_gen)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7.2.2 Using wgan with residual","metadata":{}},{"cell_type":"code","source":"syn_img_gen = SyntheticImageGenerator(wgan_res,\n                                      checkpoint_path='./checkpoints/wgan_res/checkpoints/wgan/epoch_190_13_11_22_01-48-35/wgan_ckp_epoch_190.h5',\n                                      data_dump_dir='./tmp/dump_residual')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"syn_img_gen.handle_synthetic_image_generation(disease_lst=sample_data_df['disease'].unique().tolist(),\n                                              paths_X_train_syn_gen=paths_X_train_syn_gen,\n                                              lables_y_train_syn_gen=lables_y_train_syn_gen)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  7.3 GAN Metrics ","metadata":{}},{"cell_type":"code","source":"import metrics_fid\nimport metrics_inception_score\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def copy_real_images(real_img_path_list, dst_directory='./tmp/dump_real_image'):\n\n    if len(list(os.walk(dst_directory))) > 0:\n        print(f'Found old files in {dst_directory}, all deleted')\n        shutil.rmtree(dst_directory)\n\n    for src_path in tqdm(real_img_path_list, desc='Copying files...'):\n        img_name = os.path.basename(src_path)\n        disease_name = os.path.basename(list(pb.Path(src_path).parents)[1])\n\n        _dst_directory = f'{dst_directory}/{disease_name}'\n\n        if not os.path.exists(_dst_directory):\n            os.makedirs(_dst_directory)\n\n        dst_path = os.path.join(_dst_directory, img_name)\n\n        if os.path.exists(dst_path):\n            _path, _format = os.path.splitext(dst_path)\n\n            _path = _path + '_'\n            dst_path = f'{_path}{_format}'\n\n            if os.path.exists(dst_path):\n                while os.path.exists(dst_path):\n                    _path = _path + '_'\n                    dst_path = f'{_path}{_format}'\n\n        shutil.copy(src=src_path, dst=dst_path)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"copy_real_images(paths_X_train_syn_gen)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_img_as_np_array(directory):\n\n    t1 = time.perf_counter()\n    path_lst = list(pb.Path(directory).rglob('*.jpg'))\n    buffer_arr = np.zeros([len(path_lst), 180, 180, 3], dtype='uint8')\n    for idx, path in enumerate(path_lst):\n        img_tensor = skimage.io.imread(path)  # .decode('utf-8'))\n        img_tensor = img_tensor.astype(np.float16)\n        buffer_arr[idx, :, :, :] = img_tensor\n\n    buffer_arr = buffer_arr.transpose([0, 3, 2, 1])  # [n,h,w,c] -> [n,c,h,w]\n    print(f'Shape -> {buffer_arr.shape}')\n    print(time.perf_counter() - t1)\n    return buffer_arr\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting the synthetic data as np array of vanilla wgan\nsynthetic_dataset_vanilla = load_img_as_np_array('./tmp/dump_vanilla')\n\n# getting the synthetic data as np array of wgan residual\nsynthetic_dataset_residual = load_img_as_np_array('./tmp/dump_residual')\n\n# getting the real data as np array\nreal_dataset = load_img_as_np_array('./tmp/dump_real_image')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 7.3.1 FID\n\nReference : https://github.com/tsc2017/Frechet-Inception-Distance","metadata":{}},{"cell_type":"code","source":"fid_score_vanilla = metrics_fid.get_fid(\n    images1=synthetic_dataset_vanilla, images2=real_dataset)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fid_score_residual = metrics_fid.get_fid(\n    images1=synthetic_dataset_residual, images2=real_dataset)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 7.3.2 Inception Score\n\nReference : https://github.com/tsc2017/Inception-Score","metadata":{}},{"cell_type":"code","source":"metrics_inception_score.get_inception_score(images=synthetic_dataset_vanilla)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_inception_score.get_inception_score(images=synthetic_dataset_residual)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8 Load images for final classification model training","metadata":{}},{"cell_type":"markdown","source":"### 8.1 Generate the synthetic images in the train directory","metadata":{}},{"cell_type":"code","source":"syn_img_gen_final = SyntheticImageGenerator(wgan,\n                                            checkpoint_path='./checkpoints/wgan/checkpoints/wgan/epoch_98_12_11_22_21-57-02/wgan_ckp_epoch_98.h5',\n                                            data_dump_dir='../Dataset/tea sickness dataset')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"syn_img_gen_final.handle_synthetic_image_generation(disease_lst=sample_data_df['disease'].unique().tolist(),\n                                                    paths_X_train_syn_gen=paths_X_train_syn_gen,\n                                                    lables_y_train_syn_gen=lables_y_train_syn_gen)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_synthetic_image_paths(directory, disease_list):\n    image_path_list = []\n    for disease in disease_list:\n        image_path_list_disease = list(\n            pb.Path(f'{directory}/{disease}/generated').glob('*.jpg'))\n        image_path_list.extend(image_path_list_disease)\n\n    return image_path_list\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"synthtic_images_lst = get_synthetic_image_paths(\n    '../Dataset/tea sickness dataset', actual_data_df['disease'].unique().tolist())\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.2 Get a dataframe having the disease paths and respective OHE labels","metadata":{}},{"cell_type":"code","source":"synthetic_df = get_img_path_and_labels_df(synthtic_images_lst)\nsynthetic_df\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_proportional_random_mixing(actual_data, synthetic_data, synthetic_percent=20):\n\n    print(\n        f'Shape of actual data -> {actual_data.shape}\\nShape of synthetic data -> {synthetic_data.shape}')\n    synthetic_percent = synthetic_percent/100\n    num_rows_synthetic = int(\n        np.ceil(synthetic_data.shape[0]*synthetic_percent))\n    print(f'Synthetic sample size -> {num_rows_synthetic}')\n\n    df_synthtic_sample = synthetic_data.sample(n=num_rows_synthetic)\n\n    merged_df = pd.concat([actual_data, df_synthtic_sample], axis=0)\n    print(f'After merging shape of actual data -> {merged_df.shape}')\n    return merged_df\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9 Final classification model trainig","metadata":{}},{"cell_type":"code","source":"\ndef get_proportional_mixed_data(merged_df):\n    final_paths_X_train = merged_df['paths'].values.tolist()\n    final_lables_y_train = merged_df[merged_df['disease'].unique()].values\n    final_train_dataset = gen_dataset(\n        final_paths_X_train, final_lables_y_train)\n    return final_train_dataset, final_paths_X_train\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_resnet():\n    resnet = resnet50.ResNet50(\n        include_top=False, weights='imagenet', input_shape=INPUT_SHAPE)\n    resnet_model = ModelBuilder(pretrained_model_ref=resnet, num_classes=8)\n    return resnet_model\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_SHAPE = (180, 180, 3)\nINPUT_SHAPE_BUILD = (None, 180, 180, 3)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.1 Train model with real + 20% synthetic data","metadata":{}},{"cell_type":"code","source":"merged_df = get_proportional_random_mixing(\n    actual_data_df, synthetic_df, synthetic_percent=20)\nmerged_df.head()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_model_final = get_resnet()\nmodel_helper = ModelHelper(model=resnet_model_final, LR=0.0002)\n\nresnet_model_with_syn_20 = model_helper.compile_model(optimizer_name='adam')\nresnet_model_with_syn_20.build(input_shape=INPUT_SHAPE_BUILD)\nresnet_model_with_syn_20.summary()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset, paths_X_train = get_proportional_mixed_data(merged_df)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps_per_epoch, validation_steps = get_epoch_settings(\n    paths_X_train, paths_X_val, batch_size=BATCH_SIZE_TRAIN)\nsteps_per_epoch, validation_steps\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nresnet_model_with_syn_20_history = resnet_model_with_syn_20.fit(train_dataset,\n                                                                steps_per_epoch=steps_per_epoch,\n                                                                epochs=100,\n                                                                verbose=1,\n                                                                callbacks=[\n                                                                    model_helper.decrease_lr_on_plateau_callback()],\n                                                                validation_data=val_datset,\n                                                                validation_steps=validation_steps,\n                                                                class_weight=None,\n                                                                workers=1,\n                                                                initial_epoch=0)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_helper.plot_model_train_info(resnet_model_with_syn_20_history)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_model_with_syn_20.evaluate(test_dataset)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_metrics_for_model(resnet_model_with_syn_20, 'resnet_model_with_syn_20')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump_model(resnet_model_with_syn_20,\n           'G:/Learning/Degree Courses/MS AI ML/Research/Tea Sickeness Project/Code/output', 'resnet_model_with_syn_20')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.2 Train model with real + 40% synthetic data","metadata":{}},{"cell_type":"code","source":"merged_df = get_proportional_random_mixing(\n    actual_data_df, synthetic_df, synthetic_percent=40)\nmerged_df.head()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset, paths_X_train = get_proportional_mixed_data(merged_df)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps_per_epoch, validation_steps = get_epoch_settings(\n    paths_X_train, paths_X_val, batch_size=BATCH_SIZE_TRAIN)\nsteps_per_epoch, validation_steps\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_model_final = get_resnet()\nmodel_helper = ModelHelper(model=resnet_model_final, LR=0.0002)\n\nresnet_model_with_syn_40 = model_helper.compile_model(optimizer_name='adam')\nresnet_model_with_syn_40.build(input_shape=INPUT_SHAPE_BUILD)\nresnet_model_with_syn_40.summary()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_model_with_syn_40_history = resnet_model_with_syn_40.fit(train_dataset,\n                                                                steps_per_epoch=steps_per_epoch,\n                                                                epochs=100,\n                                                                verbose=1,\n                                                                callbacks=[\n                                                                    model_helper.decrease_lr_on_plateau_callback()],\n                                                                validation_data=val_datset,\n                                                                validation_steps=validation_steps,\n                                                                class_weight=None,\n                                                                workers=1,\n                                                                initial_epoch=0)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_model_with_syn_40.evaluate(test_dataset)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_helper.plot_model_train_info(resnet_model_with_syn_40_history)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_model_with_syn_40.evaluate(test_dataset)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_metrics_for_model(resnet_model_with_syn_40, 'resnet_model_with_syn_40')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump_model(resnet_model_with_syn_40,\n           'G:/Learning/Degree Courses/MS AI ML/Research/Tea Sickeness Project/Code/output', 'resnet_model_with_syn_40')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.4 Train model with real + 100% synthetic data","metadata":{}},{"cell_type":"code","source":"merged_df = get_proportional_random_mixing(\n    actual_data_df, synthetic_df, synthetic_percent=100)\nmerged_df.head()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_model_final = get_resnet()\nmodel_helper = ModelHelper(model=resnet_model_final, LR=0.0002)\n\nresnet_model_with_syn_100 = model_helper.compile_model(optimizer_name='adam')\nresnet_model_with_syn_100.build(input_shape=INPUT_SHAPE_BUILD)\nresnet_model_with_syn_100.summary()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset, paths_X_train = get_proportional_mixed_data(merged_df)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps_per_epoch, validation_steps = get_epoch_settings(\n    paths_X_train, paths_X_val, batch_size=BATCH_SIZE_TRAIN)\nsteps_per_epoch, validation_steps\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_model_with_syn_100_history = resnet_model_with_syn_100.fit(train_dataset,\n                                                                  steps_per_epoch=steps_per_epoch,\n                                                                  epochs=100,\n                                                                  verbose=1,\n                                                                  callbacks=[\n                                                                      model_helper.decrease_lr_on_plateau_callback()],\n                                                                  validation_data=val_datset,\n                                                                  validation_steps=validation_steps,\n                                                                  class_weight=None,\n                                                                  workers=1,\n                                                                  initial_epoch=0)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_model_with_syn_100.evaluate(test_dataset)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_helper.plot_model_train_info(resnet_model_with_syn_100_history)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_metrics_for_model(resnet_model_with_syn_100, 'resnet_model_with_syn_100')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump_model(resnet_model_with_syn_100,\n           'G:/Learning/Degree Courses/MS AI ML/Research/Tea Sickeness Project/Code/output', 'resnet_model_with_syn_100')\n","metadata":{},"execution_count":null,"outputs":[]}]}